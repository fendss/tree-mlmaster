[
    [
        {
            "demo_id": "jigsaw-unintended-bias-in-toxicity-classification",
            "node_index": 2,
            "metric": null,
            "plan": "To address the challenge of detecting toxic comments while minimizing unintended\nbias, we propose a solution using adversarial training with a pre-trained\nDistilBERT model. The model will be fine-tuned to predict toxicity while\nemploying gradient reversal to reduce reliance on identity-related features.\nThis approach aims to balance performance and fairness by making the model's\nembeddings less predictive of sensitive identities. We'll evaluate using the\ncompetition's composite metric combining overall AUC and bias-adjusted\nsubmetrics.",
            "code": "import pandas as pd\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom transformers import DistilBertTokenizer, DistilBertModel, AdamW\nfrom tqdm import tqdm\nimport os\n\n# Configuration\nEVAL_IDENTITIES = [\n    \"male\",\n    \"female\",\n    \"homosexual_gay_or_lesbian\",\n    \"christian\",\n    \"jewish\",\n    \"muslim\",\n    \"black\",\n    \"white\",\n    \"psychiatric_or_mental_illness\",\n]\nMAX_LENGTH = 192\nBATCH_SIZE = 32\nEPOCHS = 2\nLR = 2e-5\nLAMBDA_ADV = 0.1\n\n# Data preparation\ntrain_df = pd.read_csv(\"input/train.csv\")\ntest_df = pd.read_csv(\"input/test.csv\")\n\n# Create binary labels\ntrain_df[\"target_bin\"] = (train_df[\"target\"] >= 0.5).astype(float)\nfor col in EVAL_IDENTITIES:\n    train_df[f\"{col}_bin\"] = (train_df[col] >= 0.5).astype(float)\n\n# Split data\ntrain_data, val_data = train_test_split(\n    train_df, test_size=0.1, stratify=train_df[\"target_bin\"], random_state=42\n)\n\n\n# Dataset classes\nclass TrainDataset(Dataset):\n    def __init__(self, df):\n        self.texts = df[\"comment_text\"].values\n        self.targets = df[\"target_bin\"].values\n        self.identities = df[[f\"{col}_bin\" for col in EVAL_IDENTITIES]].values\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = str(self.texts[idx])\n        target = self.targets[idx]\n        identity = self.identities[idx]\n        return text, torch.FloatTensor([target]), torch.FloatTensor(identity)\n\n\nclass TestDataset(Dataset):\n    def __init__(self, texts):\n        self.texts = texts\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        return str(self.texts[idx])\n\n\n# Model components\nclass GradientReversal(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x, alpha):\n        ctx.alpha = alpha\n        return x\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        return grad_output.neg() * ctx.alpha, None\n\n\nclass ToxicityModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.bert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n        self.classifier = nn.Linear(768, 1)\n        self.adversary = nn.Linear(768, len(EVAL_IDENTITIES))\n        self.grl = GradientReversal.apply\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        pooled = outputs.last_hidden_state[:, 0]\n        toxicity = self.classifier(pooled).squeeze()\n        reversed = self.grl(pooled, LAMBDA_ADV)\n        identities = self.adversary(reversed)\n        return toxicity, identities\n\n\n# Training setup\ntokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\ndef collate_fn(batch):\n    texts, targets, identities = zip(*batch)\n    encodings = tokenizer(\n        texts,\n        max_length=MAX_LENGTH,\n        padding=\"max_length\",\n        truncation=True,\n        return_tensors=\"pt\",\n    )\n    return {\n        \"input_ids\": encodings[\"input_ids\"],\n        \"attention_mask\": encodings[\"attention_mask\"],\n        \"targets\": torch.stack(targets).squeeze(),\n        \"identities\": torch.stack(identities),\n    }\n\n\ntrain_loader = DataLoader(\n    TrainDataset(train_data),\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    collate_fn=collate_fn,\n    num_workers=4,\n)\n\nval_loader = DataLoader(\n    TrainDataset(val_data),\n    batch_size=BATCH_SIZE * 2,\n    collate_fn=collate_fn,\n    num_workers=4,\n)\n\nmodel = ToxicityModel().to(device)\noptimizer = AdamW(model.parameters(), lr=LR)\ntox_loss = nn.BCEWithLogitsLoss()\nadv_loss = nn.BCEWithLogitsLoss()\n\n# Training loop\nbest_score = 0\nfor epoch in range(EPOCHS):\n    model.train()\n    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n        optimizer.zero_grad()\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        targets = batch[\"targets\"].to(device)\n        identities = batch[\"identities\"].to(device)\n\n        tox_pred, id_pred = model(input_ids, attention_mask)\n        t_loss = tox_loss(tox_pred, targets)\n        a_loss = adv_loss(id_pred, identities)\n        loss = t_loss + a_loss\n        loss.backward()\n        optimizer.step()\n\n    # Validation\n    model.eval()\n    all_preds, all_targets, all_ids = [], [], []\n    with torch.no_grad():\n        for batch in val_loader:\n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n            targets = batch[\"targets\"].cpu().numpy()\n            identities = batch[\"identities\"].cpu().numpy()\n\n            preds, _ = model(input_ids, attention_mask)\n            all_preds.extend(torch.sigmoid(preds).cpu().numpy())\n            all_targets.extend(targets)\n            all_ids.extend(identities)\n\n    # Calculate metrics\n    overall_auc = roc_auc_score(all_targets, all_preds)\n    subgroup, bpsn, bnsp = [], [], []\n    identities = np.array(all_ids)\n\n    for i in range(len(EVAL_IDENTITIES)):\n        mask = identities[:, i] > 0.5\n        if sum(mask) == 0:\n            subgroup.append(0.5)\n            bpsn.append(0.5)\n            bnsp.append(0.5)\n            continue\n\n        # Subgroup AUC\n        subgroup.append(roc_auc_score(all_targets[mask], np.array(all_preds)[mask]))\n\n        # BPSN\n        bpsn_mask = (np.array(all_targets) < 0.5 & mask) | (\n            np.array(all_targets) >= 0.5 & ~mask\n        )\n        if len(np.unique(all_targets[bpsn_mask])) > 1:\n            bpsn.append(\n                roc_auc_score(all_targets[bpsn_mask], np.array(all_preds)[bpsn_mask])\n            )\n        else:\n            bpsn.append(0.5)\n\n        # BNSP\n        bnsp_mask = (np.array(all_targets) >= 0.5 & mask) | (\n            np.array(all_targets) < 0.5 & ~mask\n        )\n        if len(np.unique(all_targets[bnsp_mask])) > 1:\n            bnsp.append(\n                roc_auc_score(all_targets[bnsp_mask], np.array(all_preds)[bnsp_mask])\n            )\n        else:\n            bnsp.append(0.5)\n\n    # Generalized mean\n    def gmean(arr, p=-5):\n        return np.power(np.mean(np.power(arr, p)), 1 / p)\n\n    final_score = (\n        0.25 * overall_auc\n        + 0.25 * gmean(subgroup)\n        + 0.25 * gmean(bpsn)\n        + 0.25 * gmean(bnsp)\n    )\n    print(f\"Validation Score: {final_score:.4f}\")\n\n    if final_score > best_score:\n        best_score = final_score\n        torch.save(model.state_dict(), \"working/best_model.pth\")\n\n# Generate predictions\nmodel.load_state_dict(torch.load(\"working/best_model.pth\"))\ntest_loader = DataLoader(\n    TestDataset(test_df[\"comment_text\"]),\n    batch_size=BATCH_SIZE * 2,\n    collate_fn=lambda x: tokenizer(\n        x,\n        max_length=MAX_LENGTH,\n        padding=\"max_length\",\n        truncation=True,\n        return_tensors=\"pt\",\n    ),\n)\n\npreds = []\nwith torch.no_grad():\n    for batch in test_loader:\n        outputs = model(\n            batch[\"input_ids\"].to(device), batch[\"attention_mask\"].to(device)\n        )[0]\n        preds.extend(torch.sigmoid(outputs).cpu().numpy())\n\nsubmission = pd.DataFrame({\"id\": test_df[\"id\"], \"prediction\": preds})\nos.makedirs(\"submission\", exist_ok=True)\nsubmission.to_csv(\"submission/submission.csv\", index=False)\nprint(f\"Final Validation Score: {best_score:.4f}\")\n",
            "analysis": "The code execution failed due to a bug. Specifically, the error occurred during the calculation of the Subgroup AUC, where a TypeError was raised because only integer scalar arrays can be converted to a scalar index. As a result, the script did not produce a 'submission.csv' file in the './submission/' directory.",
            "is_bug": "True",
            "parent_index": 0,
            "node_level_analysis": "数据划分: hold-out 80/20, random_state=42\n特征工程: 标准化(StandardScaler)对数值特征(10个)\n类别编码: one-hot→类别索引(idxmax并-1)\n类别嵌入: Wilderness(4→4维)、Soil(40→16维)\n数据管道: 张量化+DataLoader, batch=4096, shuffle=True(训练), num_workers=4\n模型构建: Embedding+ResNet-MLP, 输入Linear(10+4+16→512), 残差块序列(512→512, 512→512, 512→256, 256→256), SE(squeeze-excite r=16), 输出Linear→7, 激活GELU\n正则化: BatchNorm1d、Dropout(p=0.3/0.2)、权重衰减=0.01\n损失函数: CrossEntropyLoss\n优化与调度: AdamW lr=0.001, weight_decay=0.01; OneCycleLR max_lr=0.01, total_steps=25*len(train_loader), pct_start=0.3\n模型训练: 前向→损失→反向→更新→调度, epoch=25",
            "insights_from_parent": [
                "根节点，无需分析"
            ]
        },
        {
            "demo_id": "jigsaw-unintended-bias-in-toxicity-classification",
            "node_index": 6,
            "metric": 0.9229,
            "plan": "The error occurs due to attempting to index Python lists with boolean arrays,\nwhich is invalid. To fix this, convert lists to numpy arrays before masking.\nAlso, add missing parentheses in logical operations for BPSN/BNSP masks.\nFinally, ensure predictions are saved correctly.",
            "code": "import pandas as pd\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom transformers import DistilBertTokenizer, DistilBertModel, AdamW\nfrom tqdm import tqdm\nimport os\n\n# Configuration\nEVAL_IDENTITIES = [\n    \"male\",\n    \"female\",\n    \"homosexual_gay_or_lesbian\",\n    \"christian\",\n    \"jewish\",\n    \"muslim\",\n    \"black\",\n    \"white\",\n    \"psychiatric_or_mental_illness\",\n]\nMAX_LENGTH = 192\nBATCH_SIZE = 32\nEPOCHS = 2\nLR = 2e-5\nLAMBDA_ADV = 0.1\n\n# Data preparation\ntrain_df = pd.read_csv(\"input/train.csv\")\ntest_df = pd.read_csv(\"input/test.csv\")\n\n# Create binary labels\ntrain_df[\"target_bin\"] = (train_df[\"target\"] >= 0.5).astype(float)\nfor col in EVAL_IDENTITIES:\n    train_df[f\"{col}_bin\"] = (train_df[col].fillna(0) >= 0.5).astype(float)\n\n# Split data\ntrain_data, val_data = train_test_split(\n    train_df, test_size=0.1, stratify=train_df[\"target_bin\"], random_state=42\n)\n\n\n# Dataset classes\nclass TrainDataset(Dataset):\n    def __init__(self, df):\n        self.texts = df[\"comment_text\"].values\n        self.targets = df[\"target_bin\"].values\n        self.identities = df[[f\"{col}_bin\" for col in EVAL_IDENTITIES]].values\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        return (\n            str(self.texts[idx]),\n            torch.FloatTensor([self.targets[idx]]),\n            torch.FloatTensor(self.identities[idx]),\n        )\n\n\nclass TestDataset(Dataset):\n    def __init__(self, texts):\n        self.texts = texts\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        return str(self.texts[idx])\n\n\n# Model components\nclass GradientReversal(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x, alpha):\n        ctx.alpha = alpha\n        return x\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        return grad_output.neg() * ctx.alpha, None\n\n\nclass ToxicityModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.bert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n        self.classifier = nn.Linear(768, 1)\n        self.adversary = nn.Linear(768, len(EVAL_IDENTITIES))\n        self.grl = GradientReversal.apply\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        pooled = outputs.last_hidden_state[:, 0]\n        toxicity = self.classifier(pooled).squeeze()\n        reversed = self.grl(pooled, LAMBDA_ADV)\n        identities = self.adversary(reversed)\n        return toxicity, identities\n\n\n# Training setup\ntokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\ndef collate_fn(batch):\n    texts, targets, identities = zip(*batch)\n    encodings = tokenizer(\n        texts,\n        max_length=MAX_LENGTH,\n        padding=\"max_length\",\n        truncation=True,\n        return_tensors=\"pt\",\n    )\n    return {\n        \"input_ids\": encodings[\"input_ids\"],\n        \"attention_mask\": encodings[\"attention_mask\"],\n        \"targets\": torch.stack(targets).squeeze(),\n        \"identities\": torch.stack(identities),\n    }\n\n\ntrain_loader = DataLoader(\n    TrainDataset(train_data),\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    collate_fn=collate_fn,\n    num_workers=4,\n)\n\nval_loader = DataLoader(\n    TrainDataset(val_data),\n    batch_size=BATCH_SIZE * 2,\n    collate_fn=collate_fn,\n    num_workers=4,\n)\n\nmodel = ToxicityModel().to(device)\noptimizer = AdamW(model.parameters(), lr=LR)\ntox_loss = nn.BCEWithLogitsLoss()\nadv_loss = nn.BCEWithLogitsLoss()\n\n# Training loop\nbest_score = 0\nfor epoch in range(EPOCHS):\n    model.train()\n    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n        optimizer.zero_grad()\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        targets = batch[\"targets\"].to(device)\n        identities = batch[\"identities\"].to(device)\n\n        tox_pred, id_pred = model(input_ids, attention_mask)\n        t_loss = tox_loss(tox_pred, targets)\n        a_loss = adv_loss(id_pred, identities)\n        loss = t_loss + a_loss\n        loss.backward()\n        optimizer.step()\n\n    # Validation\n    model.eval()\n    all_preds, all_targets, all_ids = [], [], []\n    with torch.no_grad():\n        for batch in val_loader:\n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n            preds, _ = model(input_ids, attention_mask)\n\n            all_preds.extend(torch.sigmoid(preds).cpu().numpy())\n            all_targets.extend(batch[\"targets\"].numpy())\n            all_ids.extend(batch[\"identities\"].numpy())\n\n    # Convert to numpy arrays\n    all_preds = np.array(all_preds)\n    all_targets = np.array(all_targets)\n    identities = np.array(all_ids)\n\n    # Calculate metrics\n    overall_auc = roc_auc_score(all_targets, all_preds)\n    subgroup, bpsn, bnsp = [], [], []\n\n    for i in range(len(EVAL_IDENTITIES)):\n        mask = identities[:, i] > 0.5\n        if mask.sum() == 0:\n            subgroup.append(0.5)\n            bpsn.append(0.5)\n            bnsp.append(0.5)\n            continue\n\n        # Subgroup AUC\n        subgroup.append(roc_auc_score(all_targets[mask], all_preds[mask]))\n\n        # BPSN\n        bpsn_mask = ((all_targets < 0.5) & mask) | ((all_targets >= 0.5) & ~mask)\n        if len(np.unique(all_targets[bpsn_mask])) > 1:\n            bpsn.append(roc_auc_score(all_targets[bpsn_mask], all_preds[bpsn_mask]))\n        else:\n            bpsn.append(0.5)\n\n        # BNSP\n        bnsp_mask = ((all_targets >= 0.5) & mask) | ((all_targets < 0.5) & ~mask)\n        if len(np.unique(all_targets[bnsp_mask])) > 1:\n            bnsp.append(roc_auc_score(all_targets[bnsp_mask], all_preds[bnsp_mask]))\n        else:\n            bnsp.append(0.5)\n\n    # Generalized mean\n    def gmean(arr, p=-5):\n        return np.power(np.mean(np.power(arr, p)), 1 / p)\n\n    final_score = 0.25 * (overall_auc + gmean(subgroup) + gmean(bpsn) + gmean(bnsp))\n    print(f\"Validation Score: {final_score:.4f}\")\n\n    if final_score > best_score:\n        best_score = final_score\n        torch.save(model.state_dict(), \"working/best_model.pth\")\n\n# Generate predictions\nmodel.load_state_dict(torch.load(\"working/best_model.pth\"))\ntest_loader = DataLoader(\n    TestDataset(test_df[\"comment_text\"]),\n    batch_size=BATCH_SIZE * 2,\n    collate_fn=lambda x: tokenizer(\n        x,\n        max_length=MAX_LENGTH,\n        padding=\"max_length\",\n        truncation=True,\n        return_tensors=\"pt\",\n    ),\n)\n\npreds = []\nwith torch.no_grad():\n    for batch in test_loader:\n        outputs = model(\n            batch[\"input_ids\"].to(device), batch[\"attention_mask\"].to(device)\n        )[0]\n        preds.extend(torch.sigmoid(outputs).cpu().numpy())\n\nos.makedirs(\"submission\", exist_ok=True)\npd.DataFrame({\"id\": test_df[\"id\"], \"prediction\": preds}).to_csv(\n    \"submission/submission.csv\", index=False\n)\nprint(f\"Final Validation Score: {best_score:.4f}\")\n",
            "analysis": "The code successfully executed and trained a toxicity classification model using the DistilBERT transformer model. The model was evaluated on a validation set, achieving a validation score of 0.9229, indicating good performance. The predictions on the test data were saved in a 'submission.csv' file in the './submission/' directory, making it ready for submission.",
            "is_bug": "False",
            "parent_index": 2,
            "node_level_analysis": "数据划分: train_test_split 80%/20%，random_state=42\n特征工程: 数值标准化 StandardScaler（train fit，val/test transform）\n类别编码: one-hot→类别索引 via idxmax+extract（Wilderness_Area1-4, Soil_Type1-40）\n类别嵌入: Wilderness(4→4维)、Soil(40→16维)\n数据管道: 张量化为Tensor、DataLoader batch=4096、shuffle=True、num_workers=4\n模型构建: Embedding + ResNet-like MLP，输入10+4+16→512，残差块(512→512,512→512,512→256,256→256)，输出线性256→7\n正则化: BatchNorm1d、Dropout(p=0.3/0.2)、weight_decay=0.01\n损失函数: CrossEntropyLoss\n优化与调度: AdamW lr=0.001、wd=0.01；OneCycleLR max_lr=0.01、total_steps=25*len(train_loader)、pct_start=0.3\n模型训练: 前向→损失→反向→更新→调度，epoch=25，按验证准确率保存best model",
            "insights_from_parent": [
                "数据预处理：填充identity缺失值并在验证前将preds/targets/identities转为numpy以便掩码运算",
                "特征工程：保持文本tokenizer管道，明确二值化身份标签并确保张量/数组形状一致",
                "模型选择：继续使用DistilBERT主干+对抗分支，模型结构未改变",
                "参数调整：未修改学习率/批量大小等超参，修复了损失计算与张量形状匹配问题",
                "训练策略：修正布尔掩码逻辑（加括号和使用numpy索引）、修正BPSN/BNSP计算及预测保存流程，保证验证指标正确"
            ]
        }
    ],
    [
        {
            "demo_id": "jigsaw-unintended-bias-in-toxicity-classification",
            "node_index": 5,
            "metric": null,
            "plan": "To address unintended bias in toxicity detection, we propose a DeBERTa model\ntrained with a custom loss function that penalizes high toxicity predictions on\nnon-toxic comments mentioning protected identities. The model uses a weighted\ncombination of standard BCE loss and an additional MSE loss term targeting false\npositives on identity-related non-toxic examples, aiming to reduce bias while\nmaintaining overall performance.",
            "code": "import pandas as pd\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n    AdamW,\n    get_linear_schedule_with_warmup,\n)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nimport torch.nn.functional as F\n\n# Configuration\nMODEL_NAME = \"microsoft/deberta-v3-base\"\nIDENTITY_COLUMNS = [\n    \"male\",\n    \"female\",\n    \"homosexual_gay_or_lesbian\",\n    \"christian\",\n    \"jewish\",\n    \"muslim\",\n    \"black\",\n    \"white\",\n    \"psychiatric_or_mental_illness\",\n]\nBATCH_SIZE = 16\nMAX_LEN = 256\nEPOCHS = 2\nALPHA = 2.0\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Load and preprocess data\ntrain_df = pd.read_csv(\n    \"./input/train.csv\", usecols=[\"comment_text\", \"target\"] + IDENTITY_COLUMNS\n)\ntrain_df[IDENTITY_COLUMNS] = train_df[IDENTITY_COLUMNS].fillna(0.0)\ntrain_df[\"identity_mask\"] = (train_df[IDENTITY_COLUMNS] >= 0.5).any(axis=1)\ntrain_data, val_data = train_test_split(train_df, test_size=0.1, random_state=42)\n\n\n# Dataset classes\nclass TrainDataset(Dataset):\n    def __init__(self, df, tokenizer, max_len):\n        self.texts = df[\"comment_text\"].values\n        self.targets = df[\"target\"].values\n        self.identity_masks = df[\"identity_mask\"].values\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        encoding = self.tokenizer.encode_plus(\n            str(self.texts[idx]),\n            add_special_tokens=True,\n            max_length=self.max_len,\n            truncation=True,\n            padding=\"max_length\",\n            return_attention_mask=True,\n            return_tensors=\"pt\",\n        )\n        return {\n            \"input_ids\": encoding[\"input_ids\"].flatten(),\n            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n            \"target\": torch.tensor(self.targets[idx], dtype=torch.float),\n            \"identity_mask\": torch.tensor(self.identity_masks[idx], dtype=torch.bool),\n        }\n\n\nclass TestDataset(Dataset):\n    def __init__(self, texts, tokenizer, max_len):\n        self.texts = texts.values\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        encoding = self.tokenizer.encode_plus(\n            str(self.texts[idx]),\n            add_special_tokens=True,\n            max_length=self.max_len,\n            truncation=True,\n            padding=\"max_length\",\n            return_attention_mask=True,\n            return_tensors=\"pt\",\n        )\n        return {\n            \"input_ids\": encoding[\"input_ids\"].flatten(),\n            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n        }\n\n\n# Initialize model and data loaders\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nmodel = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=1).to(\n    device\n)\ntrain_loader = DataLoader(\n    TrainDataset(train_data, tokenizer, MAX_LEN),\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    num_workers=4,\n)\nval_loader = DataLoader(\n    TrainDataset(val_data, tokenizer, MAX_LEN), batch_size=BATCH_SIZE, num_workers=4\n)\n\n# Training setup\noptimizer = AdamW(model.parameters(), lr=2e-5)\ntotal_steps = len(train_loader) * EPOCHS\nscheduler = get_linear_schedule_with_warmup(\n    optimizer, num_warmup_steps=0, num_training_steps=total_steps\n)\n\n# Training loop\nfor epoch in range(EPOCHS):\n    model.train()\n    for batch in train_loader:\n        inputs = {k: v.to(device) for k, v in batch.items() if k != \"identity_mask\"}\n        identity_mask = batch[\"identity_mask\"].to(device)\n\n        outputs = model(**inputs)\n        logits = outputs.logits.squeeze()\n\n        bce_loss = F.binary_cross_entropy_with_logits(logits, inputs[\"target\"])\n\n        # Penalize high predictions for non-toxic identity comments\n        mask = (inputs[\"target\"] < 0.5) & identity_mask\n        penalty_loss = (\n            F.mse_loss(torch.sigmoid(logits[mask]), torch.zeros_like(logits[mask]))\n            if mask.sum() > 0\n            else 0\n        )\n\n        total_loss = bce_loss + ALPHA * penalty_loss\n        total_loss.backward()\n\n        optimizer.step()\n        scheduler.step()\n        optimizer.zero_grad()\n\n# Validation metrics\nmodel.eval()\nval_preds, val_targets, val_identities = [], [], []\nwith torch.no_grad():\n    for batch in val_loader:\n        inputs = {k: v.to(device) for k, v in batch.items() if k != \"identity_mask\"}\n        val_preds.extend(torch.sigmoid(model(**inputs).logits.squeeze().cpu().numpy()))\n        val_targets.extend(batch[\"target\"].cpu().numpy())\n        val_identities.extend(batch[\"identity_mask\"].cpu().numpy())\n\nval_df = pd.DataFrame(\n    {\"target\": val_targets, \"prediction\": val_preds, \"identity\": val_identities}\n)\n\n\n# Calculate competition metric components\ndef power_mean(series, p=-5):\n    return (\n        np.exp(np.mean(np.log(np.clip(series, 1e-5, None))))\n        if p == -5\n        else np.mean(series)\n    )\n\n\noverall_auc = roc_auc_score(val_df[\"target\"] >= 0.5, val_df[\"prediction\"])\n\nsubgroup_auc = (\n    roc_auc_score(\n        val_df[val_df[\"identity\"]][\"target\"] >= 0.5,\n        val_df[val_df[\"identity\"]][\"prediction\"],\n    )\n    if sum(val_df[\"identity\"]) > 0\n    else 0.5\n)\n\nbpsn_auc = roc_auc_score(\n    val_df[\n        val_df[\"identity\"] & (val_df[\"target\"] < 0.5)\n        | (~val_df[\"identity\"] & (val_df[\"target\"] >= 0.5))\n    ][\"target\"]\n    >= 0.5,\n    val_df[\n        val_df[\"identity\"] & (val_df[\"target\"] < 0.5)\n        | (~val_df[\"identity\"] & (val_df[\"target\"] >= 0.5))\n    ][\"prediction\"],\n)\n\nfinal_score = 0.25 * (\n    overall_auc + power_mean([subgroup_auc]) + power_mean([bpsn_auc]) + 0.25\n)\nprint(f\"Validation Score: {final_score:.4f}\")\n\n# Generate submission\ntest_df = pd.read_csv(\"./input/test.csv\")\ntest_loader = DataLoader(\n    TestDataset(test_df[\"comment_text\"], tokenizer, MAX_LEN),\n    batch_size=BATCH_SIZE,\n    num_workers=4,\n)\ntest_preds = []\nwith torch.no_grad():\n    for batch in test_loader:\n        inputs = {k: v.to(device) for k, v in batch.items()}\n        test_preds.extend(torch.sigmoid(model(**inputs).logits.squeeze().cpu().numpy()))\n\nsubmission = pd.DataFrame({\"id\": test_df[\"id\"], \"prediction\": test_preds})\nsubmission.to_csv(\"./submission/submission.csv\", index=False)\n",
            "analysis": "The code execution failed due to a TypeError in the model's forward function. The error indicates that an unexpected keyword argument 'target' was passed to the DebertaV2ForSequenceClassification model's forward method. As a result, the submission.csv file was not produced.",
            "is_bug": "True",
            "parent_index": 0,
            "node_level_analysis": "数据划分: hold-out 80/20, random_state=42\n特征工程: 从 one-hot 提取类别索引（Wilderness、Soil），数值特征 StandardScaler 标准化（train fit, val/test transform）\n类别嵌入: Wilderness(4→4维)、Soil(40→16维)\n数据管道: 张量化→TensorDataset, DataLoader batch=4096, shuffle=True（train）, num_workers=4\n模型构建: Embedding + ResNet-MLP（输入10+4+16→512，残差块序列512→512、512→512、512→256、256→256，SE 模块，输出256→7，激活GELU）\n正则化: BatchNorm1d、Dropout(p=0.3/0.2)、权重衰减=0.01\n损失函数: CrossEntropyLoss\n优化与调度: AdamW lr=0.001, wd=0.01；OneCycleLR max_lr=0.01, total_steps=25*len(train_loader), pct_start=0.3\n训练循环: epochs=25, 前向→损失→反向→更新→调度；基于验证准确率保存最优模型",
            "insights_from_parent": [
                "改用DeBERTa并加偏差惩罚"
            ]
        }
    ],
    [
        {
            "demo_id": "jigsaw-unintended-bias-in-toxicity-classification",
            "node_index": 3,
            "metric": null,
            "plan": "To address toxicity classification while minimizing unintended bias, we'll use a\nRoBERTa model fine-tuned with weighted loss focusing on identity-related non-\ntoxic comments. This approach emphasizes correct classification of sensitive\nsubgroups to reduce false positives, leveraging transformer capabilities for\nnuanced text understanding.",
            "code": "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW\nfrom torch.cuda.amp import autocast, GradScaler\nfrom tqdm import tqdm\n\n# Preprocess data\ntrain_df = pd.read_csv(\"./input/train.csv\")\nidentity_cols = [\n    \"male\",\n    \"female\",\n    \"homosexual_gay_or_lesbian\",\n    \"christian\",\n    \"jewish\",\n    \"muslim\",\n    \"black\",\n    \"white\",\n    \"psychiatric_or_mental_illness\",\n]\ntrain_df[identity_cols] = train_df[identity_cols].fillna(0)\nidentity_present = (train_df[identity_cols] >= 0.5).any(axis=1)\ntrain_df[\"weight\"] = np.where(identity_present & (train_df[\"target\"] < 0.5), 5.0, 1.0)\n\n# Split data\ntrain_df, val_df = train_test_split(train_df, test_size=0.1, random_state=42)\n\n# Dataset setup\ntokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\nmax_len = 192\nbatch_size = 32\n\n\nclass ToxicityDataset(Dataset):\n    def __init__(self, df):\n        self.texts = df[\"comment_text\"].values\n        self.targets = df[\"target\"].values\n        self.weights = df[\"weight\"].values\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        enc = tokenizer(\n            self.texts[idx],\n            truncation=True,\n            padding=\"max_length\",\n            max_length=max_len,\n            return_tensors=\"pt\",\n        )\n        return {\n            \"input_ids\": enc[\"input_ids\"].squeeze(),\n            \"attention_mask\": enc[\"attention_mask\"].squeeze(),\n            \"target\": torch.tensor(self.targets[idx], dtype=torch.float),\n            \"weight\": torch.tensor(self.weights[idx], dtype=torch.float),\n        }\n\n\ntrain_loader = DataLoader(\n    ToxicityDataset(train_df), batch_size=batch_size, shuffle=True, num_workers=4\n)\nval_loader = DataLoader(\n    ToxicityDataset(val_df), batch_size=batch_size * 2, num_workers=4\n)\n\n# Model setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = RobertaForSequenceClassification.from_pretrained(\n    \"roberta-base\", num_labels=1\n).to(device)\noptimizer = AdamW(model.parameters(), lr=2e-5)\nscaler = GradScaler()\n\n# Training loop\nfor epoch in range(1):\n    model.train()\n    progress = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n    for batch in progress:\n        optimizer.zero_grad()\n        with autocast():\n            outputs = model(\n                input_ids=batch[\"input_ids\"].to(device),\n                attention_mask=batch[\"attention_mask\"].to(device),\n            )\n            loss = torch.nn.functional.binary_cross_entropy_with_logits(\n                outputs.logits.squeeze(),\n                batch[\"target\"].to(device),\n                weight=batch[\"weight\"].to(device),\n            )\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n        progress.set_postfix({\"loss\": loss.item()})\n\n# Validation\nmodel.eval()\npreds, targets = [], []\nwith torch.no_grad():\n    for batch in val_loader:\n        outputs = model(\n            input_ids=batch[\"input_ids\"].to(device),\n            attention_mask=batch[\"attention_mask\"].to(device),\n        )\n        preds.extend(torch.sigmoid(outputs.logits.squeeze()).cpu().numpy())\n        targets.extend(batch[\"target\"].cpu().numpy())\nval_auc = roc_auc_score(targets, preds)\nprint(f\"Validation AUC: {val_auc:.4f}\")\n\n# Generate predictions\ntest_df = pd.read_csv(\"./input/test.csv\")\ntest_preds = []\nfor text in tqdm(test_df[\"comment_text\"]):\n    enc = tokenizer(\n        text,\n        truncation=True,\n        padding=\"max_length\",\n        max_length=max_len,\n        return_tensors=\"pt\",\n    ).to(device)\n    with torch.no_grad():\n        test_preds.append(torch.sigmoid(model(**enc).logits).item())\n\npd.DataFrame({\"id\": test_df[\"id\"], \"prediction\": test_preds}).to_csv(\n    \"./submission/submission.csv\", index=False\n)\n",
            "analysis": "The code execution failed due to a bug when calculating the ROC AUC score. The error message indicates that the continuous format is not supported, suggesting an issue with the input format for the metric calculation. Although the predictions were processed, the validation metric could not be computed, and thus, the submission.csv file could not be properly evaluated.",
            "is_bug": "True",
            "parent_index": 0,
            "node_level_analysis": "数据划分: hold-out(train_test_split, test_size=0.2, random_state=42)\n特征工程: 数值标准化(StandardScaler)；标签转换 Cover_Type→Cover_Type-1\n类别编码: one-hot→索引(Wilderness、Soil)\n类别嵌入: Wilderness(4→4维)、Soil(40→16维)\n数据管道: 张量化+DataLoader（batch=4096, shuffle=True, num_workers=4）\n模型构建: Embedding + ResNet-MLP + SE，输入线性层(10+4+16→512)，输出7类\n正则化: BatchNorm、Dropout(p=0.3/0.2)、权重衰减=0.01\n损失函数: CrossEntropyLoss\n优化与调度: AdamW(lr=0.001, wd=0.01)、OneCycleLR(max_lr=0.01, total_steps=25*len(train_loader), pct_start=0.3)\n模型训练: epoch=25，前向→损失→反向→更新→调度",
            "insights_from_parent": [
                "父节点为根节点，无需分析"
            ]
        },
        {
            "demo_id": "jigsaw-unintended-bias-in-toxicity-classification",
            "node_index": 4,
            "metric": 0.9657,
            "plan": "The previous code failed during validation because `roc_auc_score` requires\nbinary labels when using default parameters. The fix involves binarizing the\nvalidation targets using a 0.5 threshold and optimizing the test inference with\nbatched predictions. Here's the corrected implementation:",
            "code": "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW\nfrom torch.cuda.amp import autocast, GradScaler\nfrom tqdm import tqdm\n\n# Preprocess data\ntrain_df = pd.read_csv(\"./input/train.csv\")\nidentity_cols = [\n    \"male\",\n    \"female\",\n    \"homosexual_gay_or_lesbian\",\n    \"christian\",\n    \"jewish\",\n    \"muslim\",\n    \"black\",\n    \"white\",\n    \"psychiatric_or_mental_illness\",\n]\ntrain_df[identity_cols] = train_df[identity_cols].fillna(0)\nidentity_present = (train_df[identity_cols] >= 0.5).any(axis=1)\ntrain_df[\"weight\"] = np.where(identity_present & (train_df[\"target\"] < 0.5), 5.0, 1.0)\n\n# Split data\ntrain_df, val_df = train_test_split(train_df, test_size=0.1, random_state=42)\n\n# Dataset setup\ntokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\nmax_len = 192\nbatch_size = 64\n\n\nclass ToxicityDataset(Dataset):\n    def __init__(self, df):\n        self.texts = df[\"comment_text\"].values\n        self.targets = df[\"target\"].values\n        self.weights = df[\"weight\"].values\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        enc = tokenizer(\n            self.texts[idx],\n            truncation=True,\n            padding=\"max_length\",\n            max_length=max_len,\n            return_tensors=\"pt\",\n        )\n        return {\n            \"input_ids\": enc[\"input_ids\"].squeeze(),\n            \"attention_mask\": enc[\"attention_mask\"].squeeze(),\n            \"target\": torch.tensor(self.targets[idx], dtype=torch.float),\n            \"weight\": torch.tensor(self.weights[idx], dtype=torch.float),\n        }\n\n\nclass TestDataset(Dataset):\n    def __init__(self, texts):\n        self.texts = texts\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        enc = tokenizer(\n            self.texts[idx],\n            truncation=True,\n            padding=\"max_length\",\n            max_length=max_len,\n            return_tensors=\"pt\",\n        )\n        return {\n            \"input_ids\": enc[\"input_ids\"].squeeze(),\n            \"attention_mask\": enc[\"attention_mask\"].squeeze(),\n        }\n\n\n# Data loaders\ntrain_loader = DataLoader(\n    ToxicityDataset(train_df), batch_size=batch_size, shuffle=True, num_workers=4\n)\nval_loader = DataLoader(\n    ToxicityDataset(val_df), batch_size=batch_size * 2, num_workers=4\n)\n\n# Model setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = RobertaForSequenceClassification.from_pretrained(\n    \"roberta-base\", num_labels=1\n).to(device)\noptimizer = AdamW(model.parameters(), lr=2e-5)\nscaler = GradScaler()\n\n# Training\nfor epoch in range(1):\n    model.train()\n    progress = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n    for batch in progress:\n        optimizer.zero_grad()\n        with autocast():\n            outputs = model(\n                input_ids=batch[\"input_ids\"].to(device),\n                attention_mask=batch[\"attention_mask\"].to(device),\n            )\n            loss = torch.nn.functional.binary_cross_entropy_with_logits(\n                outputs.logits.squeeze(),\n                batch[\"target\"].to(device),\n                weight=batch[\"weight\"].to(device),\n            )\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n        progress.set_postfix({\"loss\": loss.item()})\n\n# Validation\nmodel.eval()\npreds, targets = [], []\nwith torch.no_grad():\n    for batch in val_loader:\n        outputs = model(\n            input_ids=batch[\"input_ids\"].to(device),\n            attention_mask=batch[\"attention_mask\"].to(device),\n        )\n        preds.extend(torch.sigmoid(outputs.logits.squeeze()).cpu().numpy())\n        targets.extend(batch[\"target\"].cpu().numpy())\n\nbinary_targets = (np.array(targets) >= 0.5).astype(int)\nval_auc = roc_auc_score(binary_targets, preds)\nprint(f\"Validation AUC: {val_auc:.4f}\")\n\n# Generate predictions\ntest_df = pd.read_csv(\"./input/test.csv\")\ntest_loader = DataLoader(\n    TestDataset(test_df[\"comment_text\"].values),\n    batch_size=batch_size * 2,\n    num_workers=4,\n)\n\nmodel.eval()\ntest_preds = []\nwith torch.no_grad():\n    for batch in tqdm(test_loader):\n        outputs = model(\n            input_ids=batch[\"input_ids\"].to(device),\n            attention_mask=batch[\"attention_mask\"].to(device),\n        )\n        test_preds.extend(torch.sigmoid(outputs.logits.squeeze()).cpu().numpy())\n\n# Save submission\nsubmission = pd.DataFrame({\"id\": test_df[\"id\"], \"prediction\": test_preds})\nsubmission.to_csv(\"./submission/submission.csv\", index=False)\n",
            "analysis": "The code successfully trained a RoBERTa-based model for toxicity classification and produced a submission file. The validation AUC achieved was 0.9657, indicating a high level of performance in distinguishing between toxic and non-toxic comments. The submission file was saved correctly to the './submission/' directory, and no bugs were observed during execution.",
            "is_bug": "False",
            "parent_index": 3,
            "node_level_analysis": "数据划分: hold-out 80/20, random_state=42\n特征工程: 数值标准化(StandardScaler)对10个数值特征\n类别编码: one-hot→类别索引(idxmax)生成 Wilderness、Soil\n类别嵌入: Wilderness(4→4维)、Soil(40→16维)\n数据管道: 张量化(TensorDataset)、DataLoader batch=4096、shuffle=True(训练)、num_workers=4\n模型构建: EmbeddingResNet—Embedding + 全连接残差块(含SE)、输入10+4+16→512、残差块至256、输出256→7\n正则化: BatchNorm1d、Dropout(p=0.3/0.3/0.2/0.2)、权重衰减=0.01\n损失函数: CrossEntropyLoss\n优化与调度: AdamW lr=0.001, wd=0.01；OneCycleLR max_lr=0.01, total_steps=25*len(train_loader), pct_start=0.3\n模型训练: 前向→损失→反向→更新→调度",
            "insights_from_parent": [
                "数据预处理：验证集标签按0.5阈值二值化以修复AUC计算",
                "特征工程：与父节点相同，未添加或修改特征工程步骤",
                "模型选择：保持使用 roberta-base 的二分类头，模型架构未改变",
                "参数调整：训练批次从32提升到64，提高训练吞吐量与显存利用",
                "训练策略：修复验证评分逻辑并新增批量化TestLoader以加速推理（替代逐条推理）"
            ]
        }
    ],
    [
        {
            "demo_id": "jigsaw-unintended-bias-in-toxicity-classification",
            "node_index": 1,
            "metric": null,
            "plan": "To address unintended bias in toxicity detection, we propose a BERT-based model\nwith adversarial debiasing. The model predicts toxicity while an adversarial\nhead predicts protected identity attributes, using gradient reversal to minimize\nbias. This approach encourages the model to learn features invariant to identity\nmentions, balancing performance and fairness.",
            "code": "import pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoModel, AutoTokenizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\n\n# Config\nMODEL_NAME = \"bert-base-uncased\"\nMAX_LENGTH = 128\nBATCH_SIZE = 32\nEPOCHS = 2\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Load data\ntrain_df = pd.read_csv(\"input/train.csv\")\ntest_df = pd.read_csv(\"input/test.csv\")\n\n# Prepare identity labels\nidentity_cols = [\n    \"male\",\n    \"female\",\n    \"homosexual_gay_or_lesbian\",\n    \"christian\",\n    \"jewish\",\n    \"muslim\",\n    \"black\",\n    \"white\",\n    \"psychiatric_or_mental_illness\",\n]\n\nfor col in identity_cols:\n    train_df[col] = (train_df[col].fillna(0) >= 0.5).astype(float)\ntrain_df[\"has_identity\"] = train_df[\"identity_annotator_count\"] > 0\n\n# Split train/validation\ntrain_data, val_data = train_test_split(\n    train_df, test_size=0.1, stratify=(train_df[\"target\"] >= 0.5), random_state=42\n)\n\n\n# Dataset class\nclass ToxicityDataset(Dataset):\n    def __init__(self, df, tokenizer):\n        self.df = df\n        self.tokenizer = tokenizer\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        text = row[\"comment_text\"]\n        target = torch.tensor(row[\"target\"], dtype=torch.float)\n        identity_labels = torch.tensor(row[identity_cols].values, dtype=torch.float)\n        has_id = torch.tensor(row[\"has_identity\"], dtype=torch.bool)\n\n        encoding = self.tokenizer(\n            text,\n            max_length=MAX_LENGTH,\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors=\"pt\",\n        )\n        return {\n            \"input_ids\": encoding[\"input_ids\"].squeeze(),\n            \"attention_mask\": encoding[\"attention_mask\"].squeeze(),\n            \"target\": target,\n            \"identity_labels\": identity_labels,\n            \"has_identity\": has_id,\n        }\n\n\n# Gradient reversal layer\nclass GradientReversal(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x, alpha):\n        ctx.alpha = alpha\n        return x\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        return grad_output.neg() * ctx.alpha, None\n\n\nclass DebiasedBERT(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.bert = AutoModel.from_pretrained(MODEL_NAME)\n        self.dropout = nn.Dropout(0.1)\n        self.toxicity_head = nn.Linear(768, 1)\n        self.adv_head = nn.Linear(768, len(identity_cols))\n\n    def forward(self, input_ids, attention_mask, alpha=0.15):\n        outputs = self.bert(input_ids, attention_mask=attention_mask)\n        pooled = outputs.last_hidden_state[:, 0, :]\n        pooled = self.dropout(pooled)\n\n        # Toxicity prediction\n        toxicity = self.toxicity_head(pooled).squeeze()\n\n        # Adversarial branch with gradient reversal\n        rev_pooled = GradientReversal.apply(pooled, alpha)\n        identities = self.adv_head(rev_pooled)\n\n        return toxicity, identities\n\n\n# Initialize components\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\ntrain_dataset = ToxicityDataset(train_data, tokenizer)\nval_dataset = ToxicityDataset(val_data, tokenizer)\n\ntrain_loader = DataLoader(\n    train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4\n)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, num_workers=4)\n\nmodel = DebiasedBERT().to(DEVICE)\nopt = torch.optim.AdamW(model.parameters(), lr=2e-5)\ntox_criterion = nn.BCEWithLogitsLoss()\nadv_criterion = nn.BCEWithLogitsLoss()\n\n# Training loop\nfor epoch in range(EPOCHS):\n    model.train()\n    for batch in train_loader:\n        input_ids = batch[\"input_ids\"].to(DEVICE)\n        attn_mask = batch[\"attention_mask\"].to(DEVICE)\n        targets = batch[\"target\"].to(DEVICE)\n        id_labels = batch[\"identity_labels\"].to(DEVICE)\n        has_id = batch[\"has_identity\"].to(DEVICE)\n\n        opt.zero_grad()\n        tox_logits, adv_logits = model(input_ids, attn_mask)\n\n        # Toxicity loss\n        tox_loss = tox_criterion(tox_logits, targets)\n\n        # Adversarial loss (only on identity-labeled samples)\n        if has_id.any():\n            adv_loss = adv_criterion(adv_logits[has_id], id_labels[has_id])\n        else:\n            adv_loss = 0\n\n        total_loss = tox_loss + 0.2 * adv_loss\n        total_loss.backward()\n        opt.step()\n\n# Validation\nmodel.eval()\nval_preds, val_targets = [], []\nwith torch.no_grad():\n    for batch in val_loader:\n        input_ids = batch[\"input_ids\"].to(DEVICE)\n        attn_mask = batch[\"attention_mask\"].to(DEVICE)\n        targets = batch[\"target\"].cpu().numpy()\n\n        logits, _ = model(input_ids, attn_mask)\n        preds = torch.sigmoid(logits).cpu().numpy()\n\n        val_preds.extend(preds)\n        val_targets.extend(targets)\n\nauc = roc_auc_score(val_targets, val_preds)\nprint(f\"Validation AUC: {auc:.4f}\")\n\n# Generate submission\ntest_dataset = ToxicityDataset(test_df, tokenizer)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, num_workers=4)\n\npredictions = []\nwith torch.no_grad():\n    for batch in test_loader:\n        input_ids = batch[\"input_ids\"].to(DEVICE)\n        attn_mask = batch[\"attention_mask\"].to(DEVICE)\n\n        logits, _ = model(input_ids, attn_mask)\n        preds = torch.sigmoid(logits).cpu().numpy()\n        predictions.extend(preds)\n\nsubmission = pd.DataFrame({\"id\": test_df[\"id\"], \"prediction\": predictions})\nsubmission.to_csv(\"submission/submission.csv\", index=False)\n",
            "analysis": "The execution of the code failed due to a TypeError in the DataLoader worker process. The error occurred when attempting to convert an ndarray of type numpy.object_ to a tensor, which is not supported. As a result, the model training could not be completed, and no submission file was generated.",
            "is_bug": "True",
            "parent_index": 0,
            "node_level_analysis": "数据划分: hold-out 80/20, random_state=42\n特征工程: 数值标准化(StandardScaler)对10个数值特征，目标变换 Cover_Type 1-7→0-6\n类别编码: one-hot→索引(Wilderness、Soil via idxmax → 索引-1)\n类别嵌入: Wilderness(4→4维)、Soil(40→16维)\n数据管道: 张量化(df→torch.float32)、TensorDataset + DataLoader(batch=4096, shuffle=True（train）、num_workers=4)\n模型构建: EmbeddingResNet，输入Linear(10+4+16→512)、残差块×4(512→512,512→512,512→256,256→256)含SE模块与shortcut，输出Linear(256→7)\n正则化: Dropout(p=0.3/0.2)、BatchNorm1d、权重衰减 wd=0.01\n损失函数: CrossEntropyLoss\n优化与调度: AdamW(lr=0.001, wd=0.01)、OneCycleLR(max_lr=0.01, total_steps=25*len(train_loader), pct_start=0.3)、epochs=25\n训练循环: 前向→损失→反向→更新→调度",
            "insights_from_parent": [
                "采用BERT并加入对抗去偏置"
            ]
        }
    ]
]