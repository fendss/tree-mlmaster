[
    [
        {
            "demo_id": "learning-agency-lab-automated-essay-scoring-2",
            "node_index": 6,
            "metric": null,
            "plan": "To solve this problem, we will use BERT embeddings to capture semantic features\nof essays and train a neural network regressor. The BERT model generates dense\nvector representations for each essay, which are then scaled and used to train a\nmultilayer perceptron (MLP). This approach leverages deep learning for feature\nextraction and regression, differing from traditional TF-IDF methods. The\nquadratic weighted kappa is computed on a validation set to evaluate model\nperformance.",
            "code": "import pandas as pd\nimport numpy as np\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.preprocessing import StandardScaler\n\n# Load data\ntrain_df = pd.read_csv(\"./input/train.csv\")\ntest_df = pd.read_csv(\"./input/test.csv\")\n\n# Split data\nX_train, X_val, y_train, y_val = train_test_split(\n    train_df[\"full_text\"], train_df[\"score\"], test_size=0.2, random_state=42\n)\n\n# Initialize BERT model\nmodel_name = \"bert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModel.from_pretrained(model_name).to(\n    torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n)\n\n\ndef get_embeddings(texts):\n    inputs = tokenizer(\n        texts.tolist(),\n        padding=True,\n        truncation=True,\n        max_length=512,\n        return_tensors=\"pt\",\n        return_attention_mask=True,\n    ).to(model.device)\n\n    with torch.no_grad():\n        outputs = model(**inputs)\n\n    # Mean pooling with attention mask\n    attention_mask = inputs[\"attention_mask\"]\n    last_hidden = outputs.last_hidden_state.masked_fill(\n        ~attention_mask[..., None].bool(), 0.0\n    )\n    return (last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]).cpu().numpy()\n\n\n# Generate embeddings\nprint(\"Processing training embeddings...\")\ntrain_emb = get_embeddings(X_train)\nprint(\"Processing validation embeddings...\")\nval_emb = get_embeddings(X_val)\nprint(\"Processing test embeddings...\")\ntest_emb = get_embeddings(test_df[\"full_text\"])\n\n# Standardize features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(train_emb)\nX_val_scaled = scaler.transform(val_emb)\nX_test_scaled = scaler.transform(test_emb)\n\n# Train model\nregressor = MLPRegressor(\n    hidden_layer_sizes=(512, 256),\n    activation=\"relu\",\n    solver=\"adam\",\n    early_stopping=True,\n    random_state=42,\n    max_iter=200,\n)\nregressor.fit(X_train_scaled, y_train)\n\n# Validate\nval_pred = regressor.predict(X_val_scaled)\nval_pred = np.clip(np.round(val_pred), 1, 6).astype(int)\nkappa = cohen_kappa_score(y_val, val_pred, weights=\"quadratic\")\nprint(f\"Validation Quadratic Weighted Kappa: {kappa:.4f}\")\n\n# Full training and prediction\nprint(\"Retraining on full dataset...\")\nfull_emb = get_embeddings(train_df[\"full_text\"])\nregressor.fit(scaler.transform(full_emb), train_df[\"score\"])\n\ntest_pred = regressor.predict(X_test_scaled)\ntest_pred = np.clip(np.round(test_pred), 1, 6).astype(int)\n\n# Save submission\npd.DataFrame({\"essay_id\": test_df[\"essay_id\"], \"score\": test_pred}).to_csv(\n    \"./submission/submission.csv\", index=False\n)\n",
            "analysis": "The code execution failed due to a CUDA out of memory error while processing embeddings with the BERT model. The GPU ran out of memory while trying to allocate additional memory for the task, leading to the termination of the process. As a result, the submission file was not generated.",
            "is_bug": "True",
            "parent_index": 0,
            "node_level_analysis": "数据划分: hold-out 80/20 (test_size=0.2, random_state=42)\n特征工程: 数值标准化(StandardScaler) 应用于十个数值特征\n类别编码: one-hot→索引 转换 Wilderness、Soil\n类别嵌入: Wilderness(4→4维)、Soil(40→16维)\n数据管道: 张量化(TensorDataset)、DataLoader batch=4096、shuffle(train)、num_workers=4\n模型构建: Embedding + ResNet-MLP；输入 Linear(10+4+16→512)+BatchNorm+GELU；残差块序列 (512→512)x2 Dropout(0.3)、(512→256) Dropout(0.2)、(256→256) Dropout(0.2)；输出 Linear(256→7)；ResidualBlock 含 SE 模块\n正则化: BatchNorm、Dropout(p=0.3/0.2)、权重衰减=0.01\n损失函数: CrossEntropyLoss\n优化与调度: AdamW(lr=0.001, wd=0.01)、OneCycleLR(max_lr=0.01, total_steps=25*len(train_loader), pct_start=0.3)\n训练循环: 前向→损失→反向→更新→调度",
            "insights_from_parent": [
                "基于BERT的语义嵌入回归"
            ]
        },
        {
            "demo_id": "learning-agency-lab-automated-essay-scoring-2",
            "node_index": 7,
            "metric": 0.722,
            "plan": "To resolve the CUDA memory error, the main issue stems from processing all texts\nat once. The solution involves: 1) Using batch processing for embedding\nextraction to reduce memory load 2) Switching to a smaller model like DistilBERT\n3) Adding memory cleanup between batches 4) Using float16 precision to reduce\nmemory usage.",
            "code": "import pandas as pd\nimport numpy as np\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.preprocessing import StandardScaler\n\n# Configuration\nBATCH_SIZE = 32\nMODEL_NAME = \"distilbert-base-uncased\"\n\n# Load data\ntrain_df = pd.read_csv(\"./input/train.csv\")\ntest_df = pd.read_csv(\"./input/test.csv\")\n\n# Split data\nX_train, X_val, y_train, y_val = train_test_split(\n    train_df[\"full_text\"], train_df[\"score\"], test_size=0.2, random_state=42\n)\n\n# Initialize model with mixed precision\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nmodel = AutoModel.from_pretrained(MODEL_NAME).to(\"cuda\")\nmodel = model.half()  # Use float16 for memory savings\n\n\ndef batch_embeddings(texts, batch_size=BATCH_SIZE):\n    embeddings = []\n    for i in range(0, len(texts), batch_size):\n        batch = texts.iloc[i : i + batch_size].tolist()\n        inputs = tokenizer(\n            batch,\n            padding=True,\n            truncation=True,\n            max_length=512,\n            return_tensors=\"pt\",\n        ).to(\"cuda\")\n\n        with torch.no_grad(), torch.cuda.amp.autocast():\n            outputs = model(**inputs).last_hidden_state\n            mask = inputs[\"attention_mask\"].unsqueeze(-1)\n            mean_emb = (outputs * mask).sum(dim=1) / mask.sum(dim=1)\n            embeddings.append(mean_emb.float().cpu().numpy())\n\n        del inputs, outputs, mask\n        torch.cuda.empty_cache()\n\n    return np.concatenate(embeddings)\n\n\n# Generate embeddings in batches\nprint(\"Processing training embeddings...\")\ntrain_emb = batch_embeddings(X_train)\nprint(\"Processing validation embeddings...\")\nval_emb = batch_embeddings(X_val)\nprint(\"Processing test embeddings...\")\ntest_emb = batch_embeddings(test_df[\"full_text\"])\n\n# Preprocessing\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(train_emb)\nX_val_scaled = scaler.transform(val_emb)\nX_test_scaled = scaler.transform(test_emb)\n\n# Model training\nregressor = MLPRegressor(\n    hidden_layer_sizes=(256, 128),\n    activation=\"relu\",\n    solver=\"adam\",\n    early_stopping=True,\n    random_state=42,\n    max_iter=200,\n)\nregressor.fit(X_train_scaled, y_train)\n\n# Validation\nval_pred = regressor.predict(X_val_scaled)\nval_pred = np.clip(np.round(val_pred), 1, 6).astype(int)\nkappa = cohen_kappa_score(y_val, val_pred, weights=\"quadratic\")\nprint(f\"Validation Quadratic Weighted Kappa: {kappa:.4f}\")\n\n# Final predictions\nprint(\"Generating test predictions...\")\ntest_pred = regressor.predict(X_test_scaled)\ntest_pred = np.clip(np.round(test_pred), 1, 6).astype(int)\n\n# Save submission\nsubmission = pd.DataFrame({\"essay_id\": test_df[\"essay_id\"], \"score\": test_pred})\nsubmission.to_csv(\"./submission/submission.csv\", index=False)\nprint(\"Submission file saved successfully!\")\n",
            "analysis": "The code successfully executed and generated a submission file named 'submission.csv' in the './submission/' directory. The model achieved a validation Quadratic Weighted Kappa score of 0.7220, indicating a good level of agreement between the predicted and actual essay scores. The metric used in this competition is the Quadratic Weighted Kappa, which is a measure that should be maximized.",
            "is_bug": "False",
            "parent_index": 6,
            "node_level_analysis": "数据划分: hold-out 20% (test_size=0.2, random_state=42)\n特征工程: 数值标准化(StandardScaler) 对10个数值特征\n类别编码: one-hot→索引(Wilderness_Area1-4→Wilderness, Soil_Type1-40→Soil)，索引0起\n类别嵌入: Wilderness(4→4维)、Soil(40→16维)\n数据管道: 张量化(TensorDataset) + DataLoader，batch=4096，shuffle=True(训练)，num_workers=4\n模型构建: EmbeddingResNet：嵌入+MLP/ResNet，input Linear(10+4+16→512)+BatchNorm1d+GELU，残差块序列(512→512,512→512,512→256,256→256)，out Linear(256→7)；ResidualBlock包含双Linear+BatchNorm1d+GELU、shortcut(if dim!=)Linear、SE模块(降维/out_dim//16→GELU→回升→Sigmoid)\n正则化: BatchNorm1d、Dropout(p=0.3/0.2)、权重衰减 wd=0.01\n损失函数: CrossEntropyLoss\n优化与调度: AdamW lr=0.001, wd=0.01；OneCycleLR max_lr=0.01, total_steps=25*len(train_loader), pct_start=0.3\n训练: epoch=25，训练循环为 前向→损失→反向→更新→调度",
            "insights_from_parent": [
                "数据预处理：改为分批(batch=32)生成嵌入，使用float16与显存清理，避免一次性OOM但精度略降",
                "特征工程：由bert-base替换为distilbert，保持语义嵌入但降低内存与推理延迟",
                "模型选择：仍用MLP回归器，但网络从(512,256)缩小到(256,128)，减小模型容量与计算开销",
                "参数调整：新增BATCH_SIZE、model.half()与autocast混合精度；调整隐藏单元以匹配较小嵌入/资源",
                "训练策略：采用批次嵌入生成并释放显存以保障可训练性，去掉对全量嵌入的重训以权衡资源与泛化"
            ]
        }
    ],
    [
        {
            "demo_id": "learning-agency-lab-automated-essay-scoring-2",
            "node_index": 1,
            "metric": 0.752,
            "plan": "To solve this problem, we will use a TF-IDF vectorizer to transform essay texts\ninto numerical features and train an XGBoost regressor to predict scores. The\nmodel's performance will be evaluated using quadratic weighted kappa on a\nvalidation set. After validation, the model is retrained on the full dataset to\ngenerate test predictions.",
            "code": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import cohen_kappa_score\nimport numpy as np\n\n# Load data\ntrain = pd.read_csv(\"./input/train.csv\")\ntest = pd.read_csv(\"./input/test.csv\")\n\n# Create train/validation split\nX_train, X_val, y_train, y_val = train_test_split(\n    train[\"full_text\"], train[\"score\"], test_size=0.2, random_state=42\n)\n\n# Text vectorization\nvectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\ntrain_features = vectorizer.fit_transform(X_train)\nval_features = vectorizer.transform(X_val)\n\n# Model training\nmodel = XGBRegressor(\n    n_estimators=300,\n    learning_rate=0.1,\n    max_depth=6,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    random_state=42,\n)\nmodel.fit(train_features, y_train)\n\n# Validation evaluation\nval_preds = model.predict(val_features)\nrounded_preds = np.round(val_preds).clip(1, 6)\nkappa = cohen_kappa_score(y_val, rounded_preds, weights=\"quadratic\")\nprint(f\"Validation Quadratic Weighted Kappa: {kappa:.4f}\")\n\n# Full training and prediction\nfinal_vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\nfull_features = final_vectorizer.fit_transform(train[\"full_text\"])\ntest_features = final_vectorizer.transform(test[\"full_text\"])\n\nfinal_model = XGBRegressor(**model.get_params())\nfinal_model.fit(full_features, train[\"score\"])\n\ntest_preds = final_model.predict(test_features)\ntest_preds_rounded = np.round(test_preds).clip(1, 6)\n\n# Generate submission\nsubmission = pd.DataFrame(\n    {\"essay_id\": test[\"essay_id\"], \"score\": test_preds_rounded.astype(int)}\n)\nsubmission.to_csv(\"./submission/submission.csv\", index=False)\n",
            "analysis": "The code successfully executed and produced a submission file in the required './submission/' directory. The model achieved a validation Quadratic Weighted Kappa score of 0.7520, indicating a good level of agreement with the actual scores. The submission file was generated correctly, and there were no bugs encountered during execution.",
            "is_bug": "False",
            "parent_index": 0,
            "node_level_analysis": "数据划分: hold-out 80/20 (test_size=0.2), random_state=42\n特征工程: 数值标准化(StandardScaler)\n类别编码: one-hot→类别索引(idxmax并减1)（Wilderness, Soil）\n类别嵌入: Wilderness(4→4维)、Soil(40→16维)\n数据管道: 张量化(Tensor)、DataLoader(batch=4096, shuffle=True（train），num_workers=4)\n模型构建: Embedding+ResNet-MLP；输入Linear(10+4+16→512)+BatchNorm+GELU，残差块×4(512→512,512→512,512→256,256→256)含SE，输出Linear(256→7)\n正则化: BatchNorm、Dropout(p=0.3/0.2)、权重衰减=0.01\n损失函数: CrossEntropyLoss\n优化与调度: AdamW(lr=0.001, wd=0.01) + OneCycleLR(max_lr=0.01, total_steps=25*len(train_loader), pct_start=0.3)",
            "insights_from_parent": [
                "无父节点，使用TF-IDF+XGBoost"
            ]
        },
        {
            "demo_id": "learning-agency-lab-automated-essay-scoring-2",
            "node_index": 2,
            "metric": 0.5615,
            "plan": "To improve the model's performance, we'll switch from TF-IDF features to BERT-\nbased embeddings for richer semantic representation. This approach leverages\npre-trained language model embeddings to capture contextual relationships in the\ntext, followed by XGBoost regression which efficiently handles the structured\nfeature space. We'll use GPU-accelerated embedding generation and XGBoost\ntraining for faster processing.",
            "code": "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import cohen_kappa_score\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\n\n# Load data\ntrain = pd.read_csv(\"./input/train.csv\")\ntest = pd.read_csv(\"./input/test.csv\")\n\n# Split data\nX_train, X_val, y_train, y_val = train_test_split(\n    train[\"full_text\"], train[\"score\"], test_size=0.2, random_state=42\n)\n\n# Initialize BERT model\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\nmodel = AutoModel.from_pretrained(\"bert-base-uncased\").to(\"cuda\")\n\n\n# Generate BERT embeddings\ndef get_embeddings(texts, batch_size=128):\n    model.eval()\n    embeddings = []\n    for i in range(0, len(texts), batch_size):\n        batch = texts[i : i + batch_size]\n        inputs = tokenizer(\n            batch, padding=True, truncation=True, max_length=256, return_tensors=\"pt\"\n        ).to(\"cuda\")\n        with torch.no_grad():\n            outputs = model(**inputs)\n        embeddings.append(outputs.last_hidden_state.mean(dim=1).cpu().numpy())\n    return np.concatenate(embeddings)\n\n\n# Create embeddings for all datasets\nprint(\"Generating embeddings...\")\ntrain_emb = get_embeddings(X_train.tolist())\nval_emb = get_embeddings(X_val.tolist())\ntest_emb = get_embeddings(test[\"full_text\"].tolist())\n\n# Train XGBoost model\nxgb = XGBRegressor(\n    n_estimators=500,\n    learning_rate=0.05,\n    max_depth=8,\n    subsample=0.8,\n    colsample_bytree=0.7,\n    tree_method=\"gpu_hist\",\n    random_state=42,\n)\nxgb.fit(train_emb, y_train)\n\n# Validate\nval_pred = xgb.predict(val_emb)\nval_pred = np.round(val_pred).clip(1, 6)\nkappa = cohen_kappa_score(y_val, val_pred, weights=\"quadratic\")\nprint(f\"Validation Quadratic Weighted Kappa: {kappa:.4f}\")\n\n# Final training and submission\nfull_emb = get_embeddings(train[\"full_text\"].tolist())\nxgb_full = XGBRegressor(**xgb.get_params())\nxgb_full.fit(full_emb, train[\"score\"])\n\ntest_pred = xgb_full.predict(test_emb)\ntest_pred = np.round(test_pred).clip(1, 6).astype(int)\n\nsubmission = pd.DataFrame({\"essay_id\": test[\"essay_id\"], \"score\": test_pred})\nsubmission.to_csv(\"./submission/submission.csv\", index=False)\n",
            "analysis": "The code successfully trains an XGBoost model using BERT embeddings to predict essay scores. The validation process yields a Quadratic Weighted Kappa score of 0.5615, indicating moderate agreement between predicted and actual scores. The predictions on the test data are saved correctly in the required 'submission.csv' file, meeting the submission requirements.",
            "is_bug": "False",
            "parent_index": 1,
            "node_level_analysis": "数据划分: hold-out 80/20, random_state=42\n特征工程: 10列数值特征StandardScaler(训练集fit_transform→验证/测试transform)、独热→类别索引(Wilderness_Area1-4→Wilderness, Soil_Type1-40→Soil)、标签Cover_Type-1→0-6\n类别嵌入: Wilderness(4→4维)、Soil(40→16维)\n数据管道: 张量化(torch.tensor) + DataLoader(batch=4096, shuffle=True(训练), num_workers=4)\n模型构建: EmbeddingResNet：输入10+4+16→Linear→512,BatchNorm1d,GELU；ResidualBlock(Linear→BatchNorm1d→GELU→Linear→BatchNorm1d) + SE(out_dim→out_dim//16→out_dim)；blocks为512→512×2( Dropout0.3)、512→256( Dropout0.2)、256→256( Dropout0.2)；输出Linear 256→7\n正则化: BatchNorm、Dropout(p=0.3/0.2)、权重衰减 wd=0.01\n损失函数: CrossEntropyLoss\n优化与调度: AdamW(lr=0.001, weight_decay=0.01) + OneCycleLR(max_lr=0.01, total_steps=25*len(train_loader), pct_start=0.3), epoch=25\n模型训练: 前向→损失→反向→更新→调度",
            "insights_from_parent": [
                "数据预处理：引入BERT tokenizer，GPU批量编码，max_length=256，取代原始TF-IDF流水线",
                "特征工程：用BERT mean-pooled密集向量替代稀疏TF-IDF特征",
                "模型选择：仍用XGBRegressor，但改为gpu_hist以利用GPU加速",
                "参数调整：增大n_estimators(→500)、降低learning_rate(→0.05)、增深树(→max_depth=8)、调整colsample/subsample",
                "训练策略：先预计算embedding再训练XGBoost，批量嵌入生成与GPU并行化提高效率"
            ]
        },
        {
            "demo_id": "learning-agency-lab-automated-essay-scoring-2",
            "node_index": 3,
            "metric": 0.789,
            "plan": "To improve the model's performance, we'll switch from using static BERT\nembeddings with XGBoost to fine-tuning a DeBERTa-v3 model end-to-end. This\nallows the transformer to learn task-specific representations and better capture\nessay scoring nuances. We'll use mixed-precision training and proper validation\nto optimize performance while maintaining efficiency.",
            "code": "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import cohen_kappa_score\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n    TrainingArguments,\n    Trainer,\n)\nimport torch\nfrom torch.utils.data import Dataset\n\n# Load data\ntrain = pd.read_csv(\"./input/train.csv\")\ntest = pd.read_csv(\"./input/test.csv\")\n\n# Split data\nX_train, X_val, y_train, y_val = train_test_split(\n    train[\"full_text\"], train[\"score\"], test_size=0.2, random_state=42\n)\n\n# Initialize DeBERTa model\nmodel_name = \"microsoft/deberta-v3-base\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=1)\n\n\n# Dataset class\nclass EssayDataset(Dataset):\n    def __init__(self, texts, scores=None):\n        self.texts = texts\n        self.scores = scores\n        self.encodings = tokenizer(\n            texts.tolist(),\n            padding=True,\n            truncation=True,\n            max_length=512,\n            return_tensors=\"pt\",\n        )\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        item = {key: val[idx] for key, val in self.encodings.items()}\n        if self.scores is not None:\n            item[\"labels\"] = torch.tensor(self.scores.iloc[idx], dtype=torch.float)\n        return item\n\n\n# Create datasets\ntrain_dataset = EssayDataset(X_train, y_train)\nval_dataset = EssayDataset(X_val, y_val)\n\n# Training configuration\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=4,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=32,\n    learning_rate=3e-5,\n    warmup_ratio=0.1,\n    weight_decay=0.01,\n    fp16=True,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    logging_dir=\"./logs\",\n    report_to=\"none\",\n)\n\n\n# Metrics calculation\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    preds = np.round(predictions.flatten()).clip(1, 6)\n    return {\"kappa\": cohen_kappa_score(labels.flatten(), preds, weights=\"quadratic\")}\n\n\n# Initialize trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    compute_metrics=compute_metrics,\n)\n\n# Train model\ntrainer.train()\n\n# Validation evaluation\nval_pred = trainer.predict(val_dataset).predictions.flatten()\nval_pred = np.round(val_pred).clip(1, 6).astype(int)\nkappa = cohen_kappa_score(y_val, val_pred, weights=\"quadratic\")\nprint(f\"Validation Quadratic Weighted Kappa: {kappa:.4f}\")\n\n# Generate test predictions\ntest_dataset = EssayDataset(test[\"full_text\"])\ntest_pred = trainer.predict(test_dataset).predictions.flatten()\ntest_pred = np.round(test_pred).clip(1, 6).astype(int)\n\n# Save submission\nsubmission = pd.DataFrame({\"essay_id\": test[\"essay_id\"], \"score\": test_pred})\nsubmission.to_csv(\"./submission/submission.csv\", index=False)\n",
            "analysis": "The code successfully trains a DeBERTa model for automated essay scoring, achieving a validation quadratic weighted kappa of 0.7890, indicating a strong agreement between predicted and actual scores. The predictions for the test data are saved in the required 'submission.csv' file in the './submission/' directory. The execution completes without any errors.",
            "is_bug": "False",
            "parent_index": 2,
            "node_level_analysis": "数据划分: hold-out 80/20, test_size=0.2, random_state=42\n特征工程: 保留10个数值特征、从one-hot提取Wilderness与Soil类别索引、数值标准化 StandardScaler（train fit, val/test transform）\n类别编码: one-hot→索引（idxmax 提取类别序号）\n类别嵌入: Wilderness(4→4维)、Soil(40→16维)\n数据管道: TensorDataset + DataLoader(batch=4096, shuffle=True（train）, num_workers=4)\n模型构建: Embedding + ResNet-MLP（input Linear(10+4+16→512)、残差块序列：512→512、512→512、512→256、256→256，SE模块），输出 Linear(256→7)，GELU 激活\n正则化: BatchNorm1d、Dropout(p=0.3/0.2)、权重衰减=0.01\n损失函数: CrossEntropyLoss\n优化与调度: AdamW(lr=0.001, wd=0.01)、OneCycleLR(max_lr=0.01, total_steps=25*len(train_loader), pct_start=0.3)\n模型训练: 前向→损失→反向→更新→调度, epoch=25, batch_size=4096",
            "insights_from_parent": [
                "数据预处理：切换到DeBERTa tokenizer，max_length增至512，封装为Dataset返回张量",
                "特征工程：取消静态预计算，转为模型端端到端学习文本表征",
                "模型选择：由XGBoost换成微调的microsoft/deberta-v3-base（AutoModelForSequenceClassification）",
                "参数调整：设置epochs=4、lr=3e-5、fp16启用、train_bs=16/eval_bs=32、warmup_ratio与weight_decay",
                "训练策略：使用transformers Trainer端到端微调、按epoch评估并load_best_model_at_end、混合精度提高训练效率"
            ]
        },
        {
            "demo_id": "learning-agency-lab-automated-essay-scoring-2",
            "node_index": 4,
            "metric": null,
            "plan": "To improve the model's performance, we'll switch from regression to ordinal\nclassification using binary cross-entropy loss across score thresholds. This\napproach better captures the ordinal nature of essay scores by predicting\nprobabilities for each threshold between scores (1-2, 2-3, ..., 5-6). The model\nwill output 5 logits representing these thresholds, and predictions will be\ndetermined by the number of thresholds crossed.",
            "code": "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import cohen_kappa_score\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n    TrainingArguments,\n    Trainer,\n)\nimport torch\nfrom torch.utils.data import Dataset\n\n# Load data\ntrain = pd.read_csv(\"./input/train.csv\")\ntest = pd.read_csv(\"./input/test.csv\")\n\n# Split data\nX_train, X_val, y_train, y_val = train_test_split(\n    train[\"full_text\"], train[\"score\"], test_size=0.2, random_state=42\n)\n\n# Initialize DeBERTa model with 5 output units for ordinal thresholds\nmodel_name = \"microsoft/deberta-v3-base\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=5)\n\n\n# Custom dataset class with ordinal label encoding\nclass EssayDataset(Dataset):\n    def __init__(self, texts, scores=None):\n        self.texts = texts\n        self.scores = scores\n        self.encodings = tokenizer(\n            texts.tolist(),\n            padding=True,\n            truncation=True,\n            max_length=512,\n            return_tensors=\"pt\",\n        )\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        item = {key: val[idx] for key, val in self.encodings.items()}\n        if self.scores is not None:\n            score = self.scores.iloc[idx]\n            # Create binary labels for ordinal thresholds 1-5\n            ordinal_labels = [1.0 if score > t else 0.0 for t in range(1, 6)]\n            item[\"labels\"] = torch.tensor(ordinal_labels, dtype=torch.float)\n        return item\n\n\n# Custom trainer for BCEWithLogitsLoss\nclass OrdinalTrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False):\n        labels = inputs.pop(\"labels\")\n        outputs = model(**inputs)\n        logits = outputs.logits\n        loss_fct = torch.nn.BCEWithLogitsLoss()\n        loss = loss_fct(logits, labels)\n        return (loss, outputs) if return_outputs else loss\n\n\n# Create datasets\ntrain_dataset = EssayDataset(X_train, y_train)\nval_dataset = EssayDataset(X_val, y_val)\n\n# Training configuration\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=4,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=32,\n    learning_rate=3e-5,\n    warmup_ratio=0.1,\n    weight_decay=0.01,\n    fp16=True,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    logging_dir=\"./logs\",\n    report_to=\"none\",\n)\n\n\n# Metric calculation for ordinal predictions\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    thresholds_passed = 1 / (1 + np.exp(-logits)) > 0.5\n    pred_scores = np.sum(thresholds_passed, axis=1) + 1\n    true_scores = np.sum(labels, axis=1) + 1\n    kappa = cohen_kappa_score(true_scores, pred_scores, weights=\"quadratic\")\n    return {\"kappa\": kappa}\n\n\n# Initialize trainer\ntrainer = OrdinalTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    compute_metrics=compute_metrics,\n)\n\n# Train model\ntrainer.train()\n\n# Generate test predictions\ntest_dataset = EssayDataset(test[\"full_text\"])\ntest_logits = trainer.predict(test_dataset).predictions\ntest_pred = np.sum(1 / (1 + np.exp(-test_logits)) > 0.5, axis=1) + 1\ntest_pred = np.clip(test_pred, 1, 6).astype(int)\n\n# Save submission\nsubmission = pd.DataFrame({\"essay_id\": test[\"essay_id\"], \"score\": test_pred})\nsubmission.to_csv(\"./submission/submission.csv\", index=False)\n\n# Final validation metrics\nval_logits = trainer.predict(val_dataset).predictions\nval_pred = np.sum(1 / (1 + np.exp(-val_logits)) > 0.5, axis=1) + 1\nval_pred = np.clip(val_pred, 1, 6).astype(int)\nkappa = cohen_kappa_score(y_val, val_pred, weights=\"quadratic\")\nprint(f\"Validation Quadratic Weighted Kappa: {kappa:.4f}\")\n",
            "analysis": "The code execution failed due to a bug. The error message indicates that the 'OrdinalTrainer.compute_loss()' method received an unexpected keyword argument 'num_items_in_batch'. This suggests an issue with the custom trainer implementation. As a result, the model training did not complete successfully, and the validation metric could not be computed. However, the code appears to be set up to save a submission file in the required format, although the actual file was not generated due to the execution failure.",
            "is_bug": "True",
            "parent_index": 3,
            "node_level_analysis": "数据划分: hold-out 80/20，random_state=42\n特征工程: 数值标准化(StandardScaler)，从one-hot提取类别索引\n类别编码: one-hot→索引(Wilderness, Soil)\n类别嵌入: Wilderness(4→4维)、Soil(40→16维)\n数据管道: 张量化 + DataLoader(batch=4096, shuffle=True, num_workers=4)，验证loader batch=4096, num_workers=4\n模型构建: Embedding(4→4维,40→16维) + 输入10+4+16→512线性+BatchNorm+GELU，残差块序列(512→512,512→512,512→256,256→256)含SE，输出256→7\n正则化: BatchNorm、Dropout(p=0.3/0.2)、权重衰减=0.01\n损失函数: CrossEntropyLoss\n优化与调度: AdamW(lr=0.001, wd=0.01) + OneCycleLR(max_lr=0.01, total_steps=25*len(train_loader), pct_start=0.3)\n模型训练: 前向→损失→反向→更新→调度，epoch=25",
            "insights_from_parent": [
                "数据预处理：将标注转为序数编码，构造5个阈值的二值标签向量（ordinal labels）",
                "特征工程：模型输出5个阈值logits，通过sigmoid阈值化后计数恢复分数",
                "模型选择：仍微调DeBERTa，但输出单位从1变为5以支持序数阈值预测",
                "参数调整：损失由回归/交叉熵改为BCEWithLogitsLoss（逐阈值二分类），训练参数沿用微调设置",
                "训练策略：自定义Trainer.compute_loss以使用BCEWithLogitsLoss，预测通过阈值计数映射为1-6，评价用QWK"
            ]
        },
        {
            "demo_id": "learning-agency-lab-automated-essay-scoring-2",
            "node_index": 5,
            "metric": 0.8066,
            "plan": "The error occurs because the custom `compute_loss` method doesn't accept\n`num_items_in_batch` argument from the parent Trainer class. To fix this, we'll\nmodify the method signature to accept arbitrary keyword arguments. Additionally,\nwe'll ensure proper ordinal regression implementation and submission file\ncreation.",
            "code": "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import cohen_kappa_score\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n    TrainingArguments,\n    Trainer,\n)\nimport torch\nfrom torch.utils.data import Dataset\n\n# Load data\ntrain = pd.read_csv(\"./input/train.csv\")\ntest = pd.read_csv(\"./input/test.csv\")\n\n# Split data\nX_train, X_val, y_train, y_val = train_test_split(\n    train[\"full_text\"], train[\"score\"], test_size=0.2, random_state=42\n)\n\n# Initialize DeBERTa model with 5 output units for ordinal thresholds\nmodel_name = \"microsoft/deberta-v3-base\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=5)\n\n\nclass EssayDataset(Dataset):\n    def __init__(self, texts, scores=None):\n        self.texts = texts\n        self.scores = scores\n        self.encodings = tokenizer(\n            texts.tolist(),\n            padding=True,\n            truncation=True,\n            max_length=512,\n            return_tensors=\"pt\",\n        )\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        item = {key: val[idx] for key, val in self.encodings.items()}\n        if self.scores is not None:\n            score = self.scores.iloc[idx]\n            ordinal_labels = [1.0 if score > t else 0.0 for t in range(1, 6)]\n            item[\"labels\"] = torch.tensor(ordinal_labels, dtype=torch.float)\n        return item\n\n\nclass OrdinalTrainer(Trainer):\n    def compute_loss(\n        self, model, inputs, return_outputs=False, **kwargs\n    ):  # Accept kwargs\n        labels = inputs.pop(\"labels\")\n        outputs = model(**inputs)\n        logits = outputs.logits\n        loss_fct = torch.nn.BCEWithLogitsLoss()\n        loss = loss_fct(logits, labels)\n        return (loss, outputs) if return_outputs else loss\n\n\n# Create datasets\ntrain_dataset = EssayDataset(X_train, y_train)\nval_dataset = EssayDataset(X_val, y_val)\n\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=3,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=16,\n    learning_rate=2e-5,\n    warmup_ratio=0.1,\n    weight_decay=0.01,\n    fp16=True,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    logging_dir=\"./logs\",\n    report_to=\"none\",\n    dataloader_num_workers=4,  # Add workers for speed\n)\n\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    thresholds_passed = 1 / (1 + np.exp(-logits)) > 0.5\n    pred_scores = np.sum(thresholds_passed, axis=1) + 1\n    true_scores = np.sum(labels, axis=1) + 1\n    kappa = cohen_kappa_score(true_scores, pred_scores, weights=\"quadratic\")\n    return {\"kappa\": kappa}\n\n\ntrainer = OrdinalTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    compute_metrics=compute_metrics,\n)\n\ntrainer.train()\n\n# Generate and save predictions\ntest_dataset = EssayDataset(test[\"full_text\"])\ntest_logits = trainer.predict(test_dataset).predictions\ntest_pred = np.sum(1 / (1 + np.exp(-test_logits)) > 0.5, axis=1) + 1\ntest_pred = np.clip(test_pred, 1, 6).astype(int)\n\npd.DataFrame({\"essay_id\": test[\"essay_id\"], \"score\": test_pred}).to_csv(\n    \"./submission/submission.csv\", index=False\n)\n\n# Calculate final validation Kappa\nval_logits = trainer.predict(val_dataset).predictions\nval_pred = np.sum(1 / (1 + np.exp(-val_logits)) > 0.5, axis=1) + 1\nkappa = cohen_kappa_score(y_val, val_pred, weights=\"quadratic\")\nprint(f\"Validation Quadratic Weighted Kappa: {kappa:.4f}\")\n",
            "analysis": "The code successfully executed and produced a submission.csv file in the correct directory. The model achieved a validation quadratic weighted kappa score of 0.8066, indicating a high level of agreement between predicted and actual scores. The goal of the competition is to maximize the quadratic weighted kappa score, making a higher score better.",
            "is_bug": "False",
            "parent_index": 4,
            "node_level_analysis": "数据划分: hold-out 80/20, random_state=42\n特征工程: 数值标准化(StandardScaler) 对10个数值字段\n类别编码: one-hot→索引(idxmax) 提取 Wilderness、Soil\n类别嵌入: Wilderness(4→4维)、Soil(40→16维)\n数据管道: 张量化→DataLoader(batch=4096, shuffle=True（train）, num_workers=4)\n模型构建: Embedding + ResNet-MLP（输入10+4+16→512, 残差块×4, SE模块）, 输出7类\n正则化: BatchNorm、Dropout(p=0.3/0.2)、权重衰减=0.01\n损失函数: CrossEntropyLoss\n优化与调度: AdamW(lr=0.001, wd=0.01)、OneCycleLR(max_lr=0.01, total_steps=25*len(train_loader), pct_start=0.3), epoch=25\n训练循环: 前向→损失→反向→更新→调度\n后处理/阈值: 预测类别+1 映射回1-7",
            "insights_from_parent": [
                "数据预处理：无显著改变，保持DeBERTa tokenizer与序数标签编码",
                "特征工程：保持阈值logits输出与sigmoid后计数的做法不变",
                "模型选择：仍为DeBERTa num_labels=5，模型结构未改",
                "参数调整：调整训练配置为epochs=3、train_bs降至8、eval_bs=16、lr降至2e-5，增加dataloader_num_workers=4",
                "训练策略：修复compute_loss签名以接受**kwargs兼容Trainer（避免num_items_in_batch错误）、改进多进程DataLoader和稳定提交生成"
            ]
        }
    ],
    [
        {
            "demo_id": "learning-agency-lab-automated-essay-scoring-2",
            "node_index": 8,
            "metric": 0.6117,
            "plan": "To solve this problem, we will use a pre-trained RoBERTa model fine-tuned for\nregression to predict essay scores. The text data will be tokenized and\nprocessed through the transformer, with a regression head outputting continuous\nscores. We'll compute quadratic weighted kappa on a validation split and\ngenerate test predictions after full training.",
            "code": "import pandas as pd\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import cohen_kappa_score\nimport os\n\n# Load data\ntrain_df = pd.read_csv(\"./input/train.csv\")\ntest_df = pd.read_csv(\"./input/test.csv\")\n\n# Split into train and validation\ntrain_data, val_data = train_test_split(train_df, test_size=0.2, random_state=42)\n\n# Tokenizer and model setup\ntokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\nmodel = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=1)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n\n# Dataset class\nclass EssayDataset(Dataset):\n    def __init__(self, texts, scores=None):\n        self.texts = texts.values\n        self.scores = scores.values if scores is not None else None\n        self.max_length = 256\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        encoding = tokenizer(\n            self.texts[idx],\n            padding=\"max_length\",\n            truncation=True,\n            max_length=self.max_length,\n            return_tensors=\"pt\",\n        )\n        item = {key: val.squeeze(0) for key, val in encoding.items()}\n        if self.scores is not None:\n            item[\"labels\"] = torch.tensor(self.scores[idx], dtype=torch.float)\n        return item\n\n\n# Create dataloaders\ntrain_dataset = EssayDataset(train_data[\"full_text\"], train_data[\"score\"])\nval_dataset = EssayDataset(val_data[\"full_text\"], val_data[\"score\"])\ntest_dataset = EssayDataset(test_df[\"full_text\"])\n\nbatch_size = 16\ntrain_loader = DataLoader(\n    train_dataset, batch_size=batch_size, shuffle=True, num_workers=4\n)\nval_loader = DataLoader(\n    val_dataset, batch_size=batch_size, shuffle=False, num_workers=4\n)\n\n# Training setup\noptimizer = AdamW(model.parameters(), lr=2e-5)\nepochs = 2\n\n# Training loop\nmodel.train()\nfor epoch in range(epochs):\n    for batch in train_loader:\n        inputs = {k: v.to(device) for k, v in batch.items()}\n        optimizer.zero_grad()\n        outputs = model(**inputs)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n\n# Validation\nmodel.eval()\nval_preds, val_labels = [], []\nwith torch.no_grad():\n    for batch in val_loader:\n        inputs = {k: v.to(device) for k, v in batch.items() if k != \"labels\"}\n        labels = batch[\"labels\"].numpy()\n        outputs = model(**inputs)\n        preds = outputs.logits.squeeze().cpu().numpy()\n        val_preds.extend(preds)\n        val_labels.extend(labels)\n\nval_preds = np.clip(np.round(val_preds), 1, 6).astype(int)\nval_labels = np.round(val_labels).astype(int)\nkappa = cohen_kappa_score(val_labels, val_preds, weights=\"quadratic\")\nprint(f\"Validation Quadratic Weighted Kappa: {kappa:.4f}\")\n\n# Full training and prediction\nfull_dataset = EssayDataset(train_df[\"full_text\"], train_df[\"score\"])\nfull_loader = DataLoader(\n    full_dataset, batch_size=batch_size, shuffle=True, num_workers=4\n)\n\nmodel = RobertaForSequenceClassification.from_pretrained(\n    \"roberta-base\", num_labels=1\n).to(device)\noptimizer = AdamW(model.parameters(), lr=2e-5)\n\nmodel.train()\nfor epoch in range(epochs):\n    for batch in full_loader:\n        inputs = {k: v.to(device) for k, v in batch.items()}\n        optimizer.zero_grad()\n        outputs = model(**inputs)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n\n# Test prediction\ntest_loader = DataLoader(\n    test_dataset, batch_size=batch_size, shuffle=False, num_workers=4\n)\ntest_preds = []\nmodel.eval()\nwith torch.no_grad():\n    for batch in test_loader:\n        inputs = {k: v.to(device) for k, v in batch.items()}\n        outputs = model(**inputs)\n        preds = outputs.logits.squeeze().cpu().numpy()\n        test_preds.extend(preds)\n\ntest_preds = np.clip(np.round(test_preds), 1, 6).astype(int)\n\n# Save submission\nos.makedirs(\"./submission\", exist_ok=True)\npd.DataFrame({\"essay_id\": test_df[\"essay_id\"], \"score\": test_preds}).to_csv(\n    \"./submission/submission.csv\", index=False\n)\n",
            "analysis": "The code successfully trains a RoBERTa model for the task of automated essay scoring and evaluates it using the quadratic weighted kappa metric. The validation kappa score achieved is 0.6117, indicating a moderate level of agreement between the predicted and actual scores. The predictions for the test set are saved in the required 'submission.csv' file in the './submission/' directory, suggesting the code executed without errors.",
            "is_bug": "False",
            "parent_index": 0,
            "node_level_analysis": "数据划分: hold-out 80/20, random_state=42\n特征工程: 数值标准化(StandardScaler) 对10个数值特征\n类别编码: One-hot → 索引(idxmax，-1) for Wilderness_Area1-4、Soil_Type1-40\n类别嵌入: Wilderness(4→4维)、Soil(40→16维)\n数据管道: 张量化(torch.tensor)、TensorDataset + DataLoader(batch=4096, shuffle=True(训练), num_workers=4)\n模型构建: EmbeddingResNet，输入 Linear(10+4+16→512)，Residual blocks: 512→512、512→512、512→256、256→256，输出 Linear(256→7)，含SE注意力(out_dim→out_dim//16→out_dim)\n正则化: BatchNorm1d、Dropout(p=0.3/0.2)、权重衰减=0.01\n损失函数: CrossEntropyLoss\n优化与调度: AdamW(lr=0.001, weight_decay=0.01)，OneCycleLR(max_lr=0.01, total_steps=25*len(train_loader), pct_start=0.3)\n模型训练: 前向→损失→反向→更新→调度, epoch=25",
            "insights_from_parent": [
                "父节点为根，无需分析"
            ]
        },
        {
            "demo_id": "learning-agency-lab-automated-essay-scoring-2",
            "node_index": 9,
            "metric": 0.7875,
            "plan": "To improve the model's performance, we'll switch from regression to\nclassification by treating the ordinal scores as discrete classes. This involves\nmodifying the model to output 6 labels, adjusting the loss function to cross-\nentropy, converting scores to 0-5 indices, and increasing training epochs for\nbetter convergence. Additionally, we'll process predictions using class\nprobabilities to better capture ordinal relationships.",
            "code": "import pandas as pd\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import (\n    RobertaTokenizer,\n    RobertaForSequenceClassification,\n    AdamW,\n    get_linear_schedule_with_warmup,\n)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import cohen_kappa_score\nimport os\n\n# Load data\ntrain_df = pd.read_csv(\"./input/train.csv\")\ntest_df = pd.read_csv(\"./input/test.csv\")\n\n# Split into train and validation\ntrain_data, val_data = train_test_split(train_df, test_size=0.2, random_state=42)\n\n# Tokenizer and model setup\ntokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\nmodel = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=6)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n\n# Dataset class with classification adjustments\nclass EssayDataset(Dataset):\n    def __init__(self, texts, scores=None):\n        self.texts = texts.values\n        self.scores = (\n            scores.values - 1 if scores is not None else None\n        )  # Convert to 0-5\n        self.max_length = 512  # Increased sequence length\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        encoding = tokenizer(\n            self.texts[idx],\n            padding=\"max_length\",\n            truncation=True,\n            max_length=self.max_length,\n            return_tensors=\"pt\",\n        )\n        item = {key: val.squeeze(0) for key, val in encoding.items()}\n        if self.scores is not None:\n            item[\"labels\"] = torch.tensor(self.scores[idx], dtype=torch.long)\n        return item\n\n\n# Create dataloaders with increased workers\nbatch_size = 16\ntrain_dataset = EssayDataset(train_data[\"full_text\"], train_data[\"score\"])\nval_dataset = EssayDataset(val_data[\"full_text\"], val_data[\"score\"])\ntest_dataset = EssayDataset(test_df[\"full_text\"])\n\ntrain_loader = DataLoader(\n    train_dataset, batch_size=batch_size, shuffle=True, num_workers=8\n)\nval_loader = DataLoader(\n    val_dataset, batch_size=batch_size, shuffle=False, num_workers=8\n)\n\n# Training setup with scheduler\noptimizer = AdamW(model.parameters(), lr=1.5e-5)\nepochs = 3\ntotal_steps = len(train_loader) * epochs\nscheduler = get_linear_schedule_with_warmup(\n    optimizer, num_warmup_steps=int(total_steps * 0.1), num_training_steps=total_steps\n)\n\n# Training loop with validation tracking\nbest_kappa = -1\nfor epoch in range(epochs):\n    model.train()\n    for batch in train_loader:\n        inputs = {k: v.to(device) for k, v in batch.items()}\n        optimizer.zero_grad()\n        outputs = model(**inputs)\n        loss = outputs.loss\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        scheduler.step()\n\n    # Validation\n    model.eval()\n    val_preds, val_labels = [], []\n    with torch.no_grad():\n        for batch in val_loader:\n            inputs = {k: v.to(device) for k, v in batch.items() if k != \"labels\"}\n            labels = batch[\"labels\"].numpy() + 1  # Convert back to 1-6\n            outputs = model(**inputs)\n            preds = outputs.logits.argmax(dim=-1).cpu().numpy() + 1\n            val_preds.extend(preds)\n            val_labels.extend(labels)\n\n    kappa = cohen_kappa_score(val_labels, val_preds, weights=\"quadratic\")\n    print(f\"Epoch {epoch+1} Validation Kappa: {kappa:.4f}\")\n    if kappa > best_kappa:\n        best_kappa = kappa\n        torch.save(model.state_dict(), \"./working/best_model.bin\")\n\n# Load best model for final training\nmodel.load_state_dict(torch.load(\"./working/best_model.bin\"))\n\n# Full training on all data with early stopping\nfull_dataset = EssayDataset(train_df[\"full_text\"], train_df[\"score\"])\nfull_loader = DataLoader(\n    full_dataset, batch_size=batch_size, shuffle=True, num_workers=8\n)\n\noptimizer = AdamW(model.parameters(), lr=1.5e-5)\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=len(full_loader),\n    num_training_steps=len(full_loader) * 2,\n)\n\nmodel.train()\nfor epoch in range(2):  # Additional fine-tuning\n    for batch in full_loader:\n        inputs = {k: v.to(device) for k, v in batch.items()}\n        optimizer.zero_grad()\n        outputs = model(**inputs)\n        loss = outputs.loss\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        scheduler.step()\n\n# Test prediction\ntest_loader = DataLoader(\n    test_dataset, batch_size=batch_size, shuffle=False, num_workers=8\n)\ntest_preds = []\nmodel.eval()\nwith torch.no_grad():\n    for batch in test_loader:\n        inputs = {k: v.to(device) for k, v in batch.items()}\n        outputs = model(**inputs)\n        preds = outputs.logits.argmax(dim=-1).cpu().numpy() + 1\n        test_preds.extend(preds)\n\n# Save submission\nos.makedirs(\"./submission\", exist_ok=True)\npd.DataFrame({\"essay_id\": test_df[\"essay_id\"], \"score\": test_preds}).to_csv(\n    \"./submission/submission.csv\", index=False\n)\n\nprint(f\"Final Validation Kappa: {best_kappa:.4f}\")\n",
            "analysis": "The training script successfully executed, producing a submission.csv file with predictions on the test data. The model achieved a final validation quadratic weighted kappa score of 0.7875, indicating a good level of agreement between the predicted and actual essay scores. The script utilized a RoBERTa model for sequence classification, fine-tuning it on the essay dataset, and demonstrated effective performance in the validation phase.",
            "is_bug": "False",
            "parent_index": 8,
            "node_level_analysis": "数据划分: hold-out 80/20，random_state=42  \n特征工程: 数值特征标准化(StandardScaler)  \n类别编码: 从独热列转索引(Wilderness_Area1-4→0-3, Soil_Type1-40→0-39)  \n类别嵌入: Wilderness(4→4维)、Soil(40→16维)  \n数据管道: 张量化→DataLoader，batch=4096，shuffle=True（train），num_workers=4  \n模型构建: Embedding + ResNet-MLP；输入Linear(10+4+16→512)+BatchNorm1d+GELU；残差块序列(512→512、512→512、512→256、256→256)；SE注意力(out_dim→out_dim//16→out_dim)；输出Linear(256→7)  \n正则化: BatchNorm1d、Dropout(p=0.3/0.2)、权重衰减=0.01  \n损失函数: CrossEntropyLoss  \n优化与调度: AdamW(lr=0.001, wd=0.01)、OneCycleLR(max_lr=0.01, total_steps=25*len(train_loader), pct_start=0.3)  \n训练: epoch=25，训练循环为 前向→损失→反向→更新→调度  \n预测/后处理: 输出argmax类别，结果+1还原到1-7",
            "insights_from_parent": [
                "数据预处理：将连续分数离散化为0-5；序列长度增至512；DataLoader workers增至8",
                "特征工程：无新增特征，侧重文本分词与标签转换",
                "模型选择：从回归头（num_labels=1）改为6类分类头（num_labels=6）",
                "参数调整：lr调为1.5e-5，引入线性warmup调度；epochs增至3；加入梯度裁剪",
                "训练策略：按验证Quadratic Kappa保存best模型，随后在全量数据上额外微调"
            ]
        },
        {
            "demo_id": "learning-agency-lab-automated-essay-scoring-2",
            "node_index": 10,
            "metric": 0.8237,
            "plan": "To improve the model's performance, we'll switch from RoBERTa-base to\nDeBERTa-v3-large, which offers enhanced attention mechanisms and better handling\nof textual nuances. This larger pretrained model should better capture essay\ncomplexity while maintaining reasonable training times given our compute\nresources. We'll adjust the tokenizer and model initialization accordingly,\nkeeping other hyperparameters consistent for initial validation.",
            "code": "import pandas as pd\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n    AdamW,\n    get_linear_schedule_with_warmup,\n)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import cohen_kappa_score\nimport os\n\n# Load data\ntrain_df = pd.read_csv(\"./input/train.csv\")\ntest_df = pd.read_csv(\"./input/test.csv\")\n\n# Split into train and validation\ntrain_data, val_data = train_test_split(train_df, test_size=0.2, random_state=42)\n\n# Initialize DeBERTa components\nmodel_name = \"microsoft/deberta-v3-large\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=6)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n\n# Dataset class remains similar with DeBERTa compatibility\nclass EssayDataset(Dataset):\n    def __init__(self, texts, scores=None):\n        self.texts = texts.values\n        self.scores = scores.values - 1 if scores is not None else None\n        self.max_length = 512\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        encoding = tokenizer(\n            self.texts[idx],\n            padding=\"max_length\",\n            truncation=True,\n            max_length=self.max_length,\n            return_tensors=\"pt\",\n        )\n        item = {key: val.squeeze(0) for key, val in encoding.items()}\n        if self.scores is not None:\n            item[\"labels\"] = torch.tensor(self.scores[idx], dtype=torch.long)\n        return item\n\n\n# Create dataloaders with optimized workers\nbatch_size = 8  # Reduced for larger model\ntrain_dataset = EssayDataset(train_data[\"full_text\"], train_data[\"score\"])\nval_dataset = EssayDataset(val_data[\"full_text\"], val_data[\"score\"])\ntest_dataset = EssayDataset(test_df[\"full_text\"])\n\ntrain_loader = DataLoader(\n    train_dataset, batch_size=batch_size, shuffle=True, num_workers=8\n)\nval_loader = DataLoader(\n    val_dataset, batch_size=batch_size, shuffle=False, num_workers=8\n)\n\n# Training setup with adjusted learning rate\noptimizer = AdamW(model.parameters(), lr=1e-5)\nepochs = 3\ntotal_steps = len(train_loader) * epochs\nscheduler = get_linear_schedule_with_warmup(\n    optimizer, num_warmup_steps=int(total_steps * 0.1), num_training_steps=total_steps\n)\n\n# Training loop with validation\nbest_kappa = -1\nfor epoch in range(epochs):\n    model.train()\n    for batch in train_loader:\n        inputs = {k: v.to(device) for k, v in batch.items()}\n        optimizer.zero_grad()\n        outputs = model(**inputs)\n        loss = outputs.loss\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        scheduler.step()\n\n    # Validation phase\n    model.eval()\n    val_preds, val_labels = [], []\n    with torch.no_grad():\n        for batch in val_loader:\n            inputs = {k: v.to(device) for k, v in batch.items() if k != \"labels\"}\n            labels = batch[\"labels\"].numpy() + 1\n            outputs = model(**inputs)\n            preds = outputs.logits.argmax(dim=-1).cpu().numpy() + 1\n            val_preds.extend(preds)\n            val_labels.extend(labels)\n\n    kappa = cohen_kappa_score(val_labels, val_preds, weights=\"quadratic\")\n    print(f\"Epoch {epoch+1} Validation Kappa: {kappa:.4f}\")\n    if kappa > best_kappa:\n        best_kappa = kappa\n        torch.save(model.state_dict(), \"./working/best_model.bin\")\n\n# Final training on full data\nmodel.load_state_dict(torch.load(\"./working/best_model.bin\"))\nfull_dataset = EssayDataset(train_df[\"full_text\"], train_df[\"score\"])\nfull_loader = DataLoader(\n    full_dataset, batch_size=batch_size, shuffle=True, num_workers=8\n)\n\noptimizer = AdamW(model.parameters(), lr=1e-6)  # Lower learning rate for fine-tuning\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=len(full_loader),\n    num_training_steps=len(full_loader) * 2,\n)\n\nmodel.train()\nfor epoch in range(2):\n    for batch in full_loader:\n        inputs = {k: v.to(device) for k, v in batch.items()}\n        optimizer.zero_grad()\n        outputs = model(**inputs)\n        loss = outputs.loss\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        scheduler.step()\n\n# Generate test predictions\ntest_loader = DataLoader(\n    test_dataset, batch_size=batch_size, shuffle=False, num_workers=8\n)\ntest_preds = []\nmodel.eval()\nwith torch.no_grad():\n    for batch in test_loader:\n        inputs = {k: v.to(device) for k, v in batch.items()}\n        outputs = model(**inputs)\n        preds = outputs.logits.argmax(dim=-1).cpu().numpy() + 1\n        test_preds.extend(preds)\n\n# Save submission\nos.makedirs(\"./submission\", exist_ok=True)\npd.DataFrame({\"essay_id\": test_df[\"essay_id\"], \"score\": test_preds}).to_csv(\n    \"./submission/submission.csv\", index=False\n)\n\nprint(f\"Final Validation Kappa: {best_kappa:.4f}\")\n",
            "analysis": "The code successfully trains a DeBERTa model to predict essay scores and achieves a validation kappa score of 0.8237, indicating good agreement with the true scores. The predictions are saved in a 'submission.csv' file in the './submission/' directory. The execution completed without any bugs, and the validation metric suggests a strong performance of the model on the validation set.",
            "is_bug": "False",
            "parent_index": 9,
            "node_level_analysis": "数据划分: hold-out 80/20，random_state=42\n数据管道: 分词(max_length=512,padding=max_length,truncation,return_tensors=pt)、张量化、DataLoader(batch=8,shuffle=True for train,shuffle=False for val/test,num_workers=8)\n类别编码: 标签减1(1-6→0-5)\n模型构建: Transformer(microsoft/deberta-v3-large) SequenceClassification，num_labels=6\n损失函数: 交叉熵\n优化与调度: AdamW(lr=1e-5)，微调 lr=1e-6；调度器 get_linear_schedule_with_warmup(warmup=10% total_steps)，微调阶段 warmup=len(full_loader)、training_steps=len(full_loader)*2\n正则化: 梯度裁剪 clip_grad_norm=1.0\n模型训练: 初始训练 epochs=3，随后全量微调 epochs=2，训练循环 前向→损失→反向→更新→调度\n后处理/阈值: logits argmax→预测索引+1",
            "insights_from_parent": [
                "数据预处理：切换为DeBERTa的tokenizer，保留max_length=512，batch_size下调以适配大模型",
                "特征工程：无改动，仍以原始分词输入为主",
                "模型选择：由RoBERTa-base替换为microsoft/deberta-v3-large以提高表示能力",
                "参数调整：batch_size降为8；初始微调lr设为1e-5，最终微调lr降至1e-6；训练轮次与调度保持类似",
                "训练策略：同样基于验证集保存best checkpoint，先小规模验证再全量微调"
            ]
        },
        {
            "demo_id": "learning-agency-lab-automated-essay-scoring-2",
            "node_index": 11,
            "metric": null,
            "plan": "To improve the model's performance by leveraging the ordinal nature of essay\nscores, we'll implement an ordinal regression approach using the CORAL method.\nThis replaces standard classification with threshold-based binary predictions,\nbetter capturing the ordered relationship between score levels. The model will\npredict probabilities for each score threshold and combine them to determine the\nfinal score.",
            "code": "import pandas as pd\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import (\n    AutoTokenizer,\n    AutoModel,\n    AdamW,\n    get_linear_schedule_with_warmup,\n)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import cohen_kappa_score\nimport os\n\n\nclass DebertaOrdinalRegression(torch.nn.Module):\n    def __init__(self, model_name):\n        super().__init__()\n        self.deberta = AutoModel.from_pretrained(model_name)\n        self.classifier = torch.nn.Linear(\n            self.deberta.config.hidden_size, 5\n        )  # 5 thresholds for 6 classes\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.deberta(input_ids=input_ids, attention_mask=attention_mask)\n        cls_output = outputs.last_hidden_state[:, 0, :]  # CLS token pooling\n        return self.classifier(cls_output)\n\n\n# Load data\ntrain_df = pd.read_csv(\"./input/train.csv\")\ntest_df = pd.read_csv(\"./input/test.csv\")\n\n# Split into train and validation\ntrain_data, val_data = train_test_split(train_df, test_size=0.2, random_state=42)\n\n# Initialize model components\nmodel_name = \"microsoft/deberta-v3-large\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = DebertaOrdinalRegression(model_name)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n\nclass EssayDataset(Dataset):\n    def __init__(self, texts, scores=None):\n        self.texts = texts.values\n        self.scores = (\n            scores.values - 1 if scores is not None else None\n        )  # Convert to 0-5\n        self.max_length = 512\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        encoding = tokenizer(\n            self.texts[idx],\n            padding=\"max_length\",\n            truncation=True,\n            max_length=self.max_length,\n            return_tensors=\"pt\",\n        )\n        item = {key: val.squeeze(0) for key, val in encoding.items()}\n        if self.scores is not None:\n            item[\"labels\"] = torch.tensor(self.scores[idx], dtype=torch.long)\n        return item\n\n\n# Create dataloaders\nbatch_size = 8\ntrain_dataset = EssayDataset(train_data[\"full_text\"], train_data[\"score\"])\nval_dataset = EssayDataset(val_data[\"full_text\"], val_data[\"score\"])\ntest_dataset = EssayDataset(test_df[\"full_text\"])\n\ntrain_loader = DataLoader(\n    train_dataset, batch_size=batch_size, shuffle=True, num_workers=8\n)\nval_loader = DataLoader(\n    val_dataset, batch_size=batch_size, shuffle=False, num_workers=8\n)\n\n# Training setup\noptimizer = AdamW(model.parameters(), lr=1e-5)\nepochs = 3\ntotal_steps = len(train_loader) * epochs\nscheduler = get_linear_schedule_with_warmup(\n    optimizer, num_warmup_steps=int(total_steps * 0.1), num_training_steps=total_steps\n)\ncriterion = torch.nn.BCEWithLogitsLoss()\n\nbest_kappa = -1\nfor epoch in range(epochs):\n    model.train()\n    for batch in train_loader:\n        inputs = {k: v.to(device) for k, v in batch.items() if k != \"labels\"}\n        labels = batch[\"labels\"].to(device)\n\n        optimizer.zero_grad()\n        logits = model(**inputs)\n        targets = (labels.unsqueeze(1) > torch.arange(5, device=device)).float()\n        loss = criterion(logits, targets)\n\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        scheduler.step()\n\n    # Validation\n    model.eval()\n    val_preds, val_labels = [], []\n    with torch.no_grad():\n        for batch in val_loader:\n            inputs = {k: v.to(device) for k, v in batch.items() if k != \"labels\"}\n            labels = batch[\"labels\"].cpu().numpy() + 1  # Convert back to 1-6\n\n            logits = model(**inputs)\n            probabilities = torch.sigmoid(logits)\n            preds = (probabilities > 0.5).sum(dim=1).cpu().numpy() + 1\n\n            val_preds.extend(preds)\n            val_labels.extend(labels)\n\n    kappa = cohen_kappa_score(val_labels, val_preds, weights=\"quadratic\")\n    print(f\"Epoch {epoch+1} Validation Kappa: {kappa:.4f}\")\n    if kappa > best_kappa:\n        best_kappa = kappa\n        torch.save(model.state_dict(), \"./working/best_model.bin\")\n\n# Final training on full data\nmodel.load_state_dict(torch.load(\"./working/best_model.bin\"))\nfull_dataset = EssayDataset(train_df[\"full_text\"], train_df[\"score\"])\nfull_loader = DataLoader(\n    full_dataset, batch_size=batch_size, shuffle=True, num_workers=8\n)\n\noptimizer = AdamW(model.parameters(), lr=1e-6)\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=len(full_loader),\n    num_training_steps=len(full_loader) * 2,\n)\n\nmodel.train()\nfor epoch in range(2):\n    for batch in full_loader:\n        inputs = {k: v.to(device) for k, v in batch.items() if k != \"labels\"}\n        labels = batch[\"labels\"].to(device)\n\n        optimizer.zero_grad()\n        logits = model(**inputs)\n        targets = (labels.unsqueeze(1) > torch.arange(5, device=device)).float()\n        loss = criterion(logits, targets)\n\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        scheduler.step()\n\n# Generate test predictions\ntest_loader = DataLoader(\n    test_dataset, batch_size=batch_size, shuffle=False, num_workers=8\n)\ntest_preds = []\nmodel.eval()\nwith torch.no_grad():\n    for batch in test_loader:\n        inputs = {k: v.to(device) for k, v in batch.items()}\n        logits = model(**inputs)\n        probabilities = torch.sigmoid(logits)\n        preds = (probabilities > 0.5).sum(dim=1).cpu().numpy() + 1\n        test_preds.extend(preds)\n\n# Save submission\nos.makedirs(\"./submission\", exist_ok=True)\npd.DataFrame({\"essay_id\": test_df[\"essay_id\"], \"score\": test_preds}).to_csv(\n    \"./submission/submission.csv\", index=False\n)\n\nprint(f\"Final Validation Kappa: {best_kappa:.4f}\")\n",
            "analysis": "The execution of the training script failed due to a TypeError. The error occurred because the DebertaOrdinalRegression model's forward method received an unexpected keyword argument 'token_type_ids'. This issue needs to be resolved for the script to run successfully and produce predictions. However, the script is designed to save the predictions in a 'submission.csv' file in the './submission/' directory, which indicates that the submission file would be generated if the script executed successfully.",
            "is_bug": "True",
            "parent_index": 10,
            "node_level_analysis": "数据划分: hold-out 20%（test_size=0.2, random_state=42）\n特征工程: 数值标准化(StandardScaler) 对10个数值特征\n类别编码: 从一热列通过 idxmax 提取类别索引（Wilderness_Area1-4→0-3, Soil_Type1-40→0-39）\n类别嵌入: Wilderness(4→4维)、Soil(40→16维)\n数据管道: 张量化+DataLoader（batch=4096, shuffle=True(训练), num_workers=4）\n模型构建: Embedding + ResNet-MLP，输入10+4+16→512, 残差块序列(512→512×2 dropout=0.3, 512→256 dropout=0.2, 256→256 dropout=0.2)，SE通道注意，输出256→7，激活GELU\n正则化: BatchNorm1d、Dropout(p=0.3/0.2)、权重衰减=0.01\n损失函数: CrossEntropyLoss\n优化与调度: AdamW(lr=1e-3, wd=0.01) + OneCycleLR(max_lr=1e-2, total_steps=25*len(train_loader), pct_start=0.3)\n模型训练: 前向→损失→反向→更新→调度，epoch=25",
            "insights_from_parent": [
                "数据预处理：标签仍为0-5，但训练目标从类别标签改为阈值二元矩阵（对每阈值构造二元目标）",
                "特征工程：将标签重构为5个阈值的二元目标以反映序数关系",
                "模型选择：用自定义Ordinal head（AutoModel+5个阈值输出）替代标准SequenceClassification头",
                "参数调整：损失由交叉熵改为BCEWithLogits以对阈值进行二元回归；保留AdamW、scheduler与梯度裁剪",
                "训练策略：按阈值概率(>0.5)累计预测最终分数，旨在更好建模分数的序数性质并保存最优模型"
            ]
        },
        {
            "demo_id": "learning-agency-lab-automated-essay-scoring-2",
            "node_index": 12,
            "metric": 0.8207,
            "plan": "The error occurs because the DeBERTa model's forward method receives unexpected\n`token_type_ids` from the tokenizer. To fix this, we modify the tokenizer to\ndisable token_type_ids generation and adjust the model's forward method to\nignore extra parameters. This ensures compatibility between the tokenizer output\nand model inputs.",
            "code": "import pandas as pd\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import (\n    AutoTokenizer,\n    AutoModel,\n    AdamW,\n    get_linear_schedule_with_warmup,\n)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import cohen_kappa_score\nimport os\n\n\nclass DebertaOrdinalRegression(torch.nn.Module):\n    def __init__(self, model_name):\n        super().__init__()\n        self.deberta = AutoModel.from_pretrained(model_name)\n        self.classifier = torch.nn.Linear(\n            self.deberta.config.hidden_size, 5\n        )  # 5 thresholds for 6 classes\n\n    def forward(self, input_ids, attention_mask, **kwargs):\n        outputs = self.deberta(input_ids=input_ids, attention_mask=attention_mask)\n        cls_output = outputs.last_hidden_state[:, 0, :]  # CLS token pooling\n        return self.classifier(cls_output)\n\n\n# Load data\ntrain_df = pd.read_csv(\"./input/train.csv\")\ntest_df = pd.read_csv(\"./input/test.csv\")\n\n# Split into train and validation\ntrain_data, val_data = train_test_split(train_df, test_size=0.2, random_state=42)\n\n# Initialize model components\nmodel_name = \"microsoft/deberta-v3-large\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = DebertaOrdinalRegression(model_name)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n\nclass EssayDataset(Dataset):\n    def __init__(self, texts, scores=None):\n        self.texts = texts.values\n        self.scores = (\n            scores.values - 1 if scores is not None else None\n        )  # Convert to 0-5\n        self.max_length = 512\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        encoding = tokenizer(\n            self.texts[idx],\n            padding=\"max_length\",\n            truncation=True,\n            max_length=self.max_length,\n            return_tensors=\"pt\",\n            return_token_type_ids=False,  # Disable token_type_ids\n        )\n        item = {key: val.squeeze(0) for key, val in encoding.items()}\n        if self.scores is not None:\n            item[\"labels\"] = torch.tensor(self.scores[idx], dtype=torch.long)\n        return item\n\n\n# Create dataloaders\nbatch_size = 8\ntrain_dataset = EssayDataset(train_data[\"full_text\"], train_data[\"score\"])\nval_dataset = EssayDataset(val_data[\"full_text\"], val_data[\"score\"])\ntest_dataset = EssayDataset(test_df[\"full_text\"])\n\ntrain_loader = DataLoader(\n    train_dataset, batch_size=batch_size, shuffle=True, num_workers=8\n)\nval_loader = DataLoader(\n    val_dataset, batch_size=batch_size, shuffle=False, num_workers=8\n)\n\n# Training setup\noptimizer = AdamW(model.parameters(), lr=1e-5)\nepochs = 3\ntotal_steps = len(train_loader) * epochs\nscheduler = get_linear_schedule_with_warmup(\n    optimizer, num_warmup_steps=int(total_steps * 0.1), num_training_steps=total_steps\n)\ncriterion = torch.nn.BCEWithLogitsLoss()\n\nbest_kappa = -1\nfor epoch in range(epochs):\n    model.train()\n    for batch in train_loader:\n        inputs = {k: v.to(device) for k, v in batch.items() if k != \"labels\"}\n        labels = batch[\"labels\"].to(device)\n\n        optimizer.zero_grad()\n        logits = model(**inputs)\n        targets = (labels.unsqueeze(1) > torch.arange(5, device=device)).float()\n        loss = criterion(logits, targets)\n\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        scheduler.step()\n\n    # Validation\n    model.eval()\n    val_preds, val_labels = [], []\n    with torch.no_grad():\n        for batch in val_loader:\n            inputs = {k: v.to(device) for k, v in batch.items() if k != \"labels\"}\n            labels = batch[\"labels\"].cpu().numpy() + 1  # Convert back to 1-6\n\n            logits = model(**inputs)\n            probabilities = torch.sigmoid(logits)\n            preds = (probabilities > 0.5).sum(dim=1).cpu().numpy() + 1\n\n            val_preds.extend(preds)\n            val_labels.extend(labels)\n\n    kappa = cohen_kappa_score(val_labels, val_preds, weights=\"quadratic\")\n    print(f\"Epoch {epoch+1} Validation Kappa: {kappa:.4f}\")\n    if kappa > best_kappa:\n        best_kappa = kappa\n        torch.save(model.state_dict(), \"./working/best_model.bin\")\n\n# Final training on full data\nmodel.load_state_dict(torch.load(\"./working/best_model.bin\"))\nfull_dataset = EssayDataset(train_df[\"full_text\"], train_df[\"score\"])\nfull_loader = DataLoader(\n    full_dataset, batch_size=batch_size, shuffle=True, num_workers=8\n)\n\noptimizer = AdamW(model.parameters(), lr=1e-6)\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=len(full_loader),\n    num_training_steps=len(full_loader) * 2,\n)\n\nmodel.train()\nfor epoch in range(2):\n    for batch in full_loader:\n        inputs = {k: v.to(device) for k, v in batch.items() if k != \"labels\"}\n        labels = batch[\"labels\"].to(device)\n\n        optimizer.zero_grad()\n        logits = model(**inputs)\n        targets = (labels.unsqueeze(1) > torch.arange(5, device=device)).float()\n        loss = criterion(logits, targets)\n\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        scheduler.step()\n\n# Generate test predictions\ntest_loader = DataLoader(\n    test_dataset, batch_size=batch_size, shuffle=False, num_workers=8\n)\ntest_preds = []\nmodel.eval()\nwith torch.no_grad():\n    for batch in test_loader:\n        inputs = {k: v.to(device) for k, v in batch.items()}\n        logits = model(**inputs)\n        probabilities = torch.sigmoid(logits)\n        preds = (probabilities > 0.5).sum(dim=1).cpu().numpy() + 1\n        test_preds.extend(preds)\n\n# Save submission\nos.makedirs(\"./submission\", exist_ok=True)\npd.DataFrame({\"essay_id\": test_df[\"essay_id\"], \"score\": test_preds}).to_csv(\n    \"./submission/submission.csv\", index=False\n)\n\nprint(f\"Final Validation Kappa: {best_kappa:.4f}\")\n",
            "analysis": "The training script executed successfully, producing a submission.csv file with predictions for the test data. The model achieved a final validation Kappa score of 0.8207, indicating a high level of agreement between the predicted and actual essay scores. The Kappa metric is to be maximized in this competition, and the achieved score suggests a strong performance by the model.",
            "is_bug": "False",
            "parent_index": 11,
            "node_level_analysis": "数据划分: hold-out 80/20, random_state=42\n特征工程: 数值标准化(StandardScaler)对10个数值列\n类别编码: one-hot→序数索引(0-based) for Wilderness_Area(1-4)、Soil_Type(1-40)\n类别嵌入: Wilderness(4→4维)、Soil(40→16维)\n数据管道: 张量化(df→torch.tensor)、DataLoader batch=4096、shuffle(train)=True、num_workers=4\n模型构建: Embedding + ResNet-MLP；输入 Linear(10+4+16→512)、BatchNorm1d、GELU；blocks: 512→512(dropout0.3)、512→512(dropout0.3)、512→256(dropout0.2)、256→256(dropout0.2)；ResidualBlock含残差短路与SE(se: Linear(out→out//16)→GELU→Linear→Sigmoid)；输出 Linear(256→7)\n正则化: BatchNorm1d、Dropout(p=0.3/0.2)、权重衰减 wd=0.01\n损失函数: CrossEntropyLoss\n优化与调度: AdamW(lr=0.001, wd=0.01)、OneCycleLR(max_lr=0.01, total_steps=25*len(train_loader), pct_start=0.3)\n模型训练: 前向→损失→反向→更新→调度, epoch=25, 验证集上保存最佳模型(checkpoint)",
            "insights_from_parent": [
                "数据预处理：tokenizer调用禁止生成token_type_ids（return_token_type_ids=False）以避免多余字段",
                "特征工程：无变化，仍使用相同分词与标签格式",
                "模型选择：模型forward增加 **kwargs 接收以兼容多余输入字段",
                "参数调整：未改动超参数，主要修复接口兼容性",
                "训练策略：修复输入兼容性错误后继续原有训练/验证流程，保持CORAL训练方案"
            ]
        }
    ],
    [
        {
            "demo_id": "learning-agency-lab-automated-essay-scoring-2",
            "node_index": 13,
            "metric": 0.7629571687596535,
            "plan": "To solve this problem, we will leverage a combination of TF-IDF features and\npre-trained BERT embeddings to capture both syntactic and semantic information\nfrom essays. These combined features will be used to train a LightGBM regressor,\nproviding a robust model that benefits from traditional and deep learning\napproaches. Validation will be performed using quadratic weighted kappa to\nensure alignment with the competition metric.",
            "code": "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import cohen_kappa_score\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\nfrom lightgbm import LGBMRegressor\n\n# Load data\ntrain_df = pd.read_csv(\"./input/train.csv\")\ntest_df = pd.read_csv(\"./input/test.csv\")\n\n# Precompute BERT embeddings for all essays\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\nmodel = AutoModel.from_pretrained(\"bert-base-uncased\").to(\"cuda\")\n\n\ndef get_bert_embeddings(texts, batch_size=32):\n    model.eval()\n    embeddings = []\n    for i in range(0, len(texts), batch_size):\n        batch = texts[i : i + batch_size]\n        inputs = tokenizer(\n            batch, padding=True, truncation=True, max_length=512, return_tensors=\"pt\"\n        ).to(\"cuda\")\n        with torch.no_grad():\n            outputs = model(**inputs)\n        last_hidden = outputs.last_hidden_state\n        mask = inputs[\"attention_mask\"].unsqueeze(-1).expand(last_hidden.size()).float()\n        mean_embeddings = torch.sum(last_hidden * mask, 1) / torch.clamp(\n            mask.sum(1), min=1e-9\n        )\n        embeddings.append(mean_embeddings.cpu().numpy())\n    return np.concatenate(embeddings, axis=0)\n\n\nall_texts = pd.concat([train_df[\"full_text\"], test_df[\"full_text\"]]).tolist()\nall_bert = get_bert_embeddings(all_texts)\ntrain_bert = all_bert[: len(train_df)]\ntest_bert = all_bert[len(train_df) :]\n\n# Split training data and get indices\nX_train_part, X_val_part, y_train_part, y_val_part = train_test_split(\n    train_df[\"full_text\"], train_df[\"score\"], test_size=0.2, random_state=42\n)\ntrain_indices = X_train_part.index\nval_indices = X_val_part.index\n\n# Validation phase TF-IDF\ntfidf_part = TfidfVectorizer(max_features=10000)\nX_train_tfidf = tfidf_part.fit_transform(X_train_part)\nX_val_tfidf = tfidf_part.transform(X_val_part)\n\n# Combine features for validation\nX_train_combined = np.hstack([X_train_tfidf.toarray(), train_bert[train_indices]])\nX_val_combined = np.hstack([X_val_tfidf.toarray(), train_bert[val_indices]])\n\n# Train and validate\nlgb = LGBMRegressor(n_estimators=500, random_state=42)\nlgb.fit(X_train_combined, y_train_part)\nval_preds = lgb.predict(X_val_combined)\nval_preds = np.clip(np.round(val_preds), 1, 6).astype(int)\nkappa = cohen_kappa_score(y_val_part, val_preds, weights=\"quadratic\")\nprint(f\"Validation Quadratic Weighted Kappa: {kappa}\")\n\n# Full training\ntfidf_full = TfidfVectorizer(max_features=10000)\nX_full_tfidf = tfidf_full.fit_transform(train_df[\"full_text\"])\nX_test_tfidf = tfidf_full.transform(test_df[\"full_text\"])\n\nX_full_combined = np.hstack([X_full_tfidf.toarray(), train_bert])\nX_test_combined = np.hstack([X_test_tfidf.toarray(), test_bert])\n\nlgb_full = LGBMRegressor(n_estimators=500, random_state=42)\nlgb_full.fit(X_full_combined, train_df[\"score\"])\ntest_preds = lgb_full.predict(X_test_combined)\ntest_preds = np.clip(np.round(test_preds), 1, 6).astype(int)\n\n# Create submission\npd.DataFrame({\"essay_id\": test_df[\"essay_id\"], \"score\": test_preds}).to_csv(\n    \"./submission/submission.csv\", index=False\n)\n",
            "analysis": "The code successfully executed and produced a submission file in the correct format. The model achieved a validation Quadratic Weighted Kappa score of 0.7629, indicating a good level of agreement between the predicted and actual essay scores. The metric used, Quadratic Weighted Kappa, should be maximized as higher values indicate better performance.",
            "is_bug": "False",
            "parent_index": 0,
            "node_level_analysis": "数据划分: hold-out 80/20, random_state=42\n特征工程: 数值标准化(StandardScaler)，train fit_transform、val/test transform\n类别编码: one-hot→类别索引 via idxmax+提取数字 (Wilderness_Area1-4→Wilderness, Soil_Type1-40→Soil)\n类别嵌入: Wilderness(4→4维)、Soil(40→16维)\n数据管道: 张量化(TensorDataset)、DataLoader(train: batch=4096, shuffle=True, num_workers=4)、DataLoader(val: batch=4096, num_workers=4096)\n模型构建: Embedding + ResNet-MLP + SE，输入Linear(10+4+16→512)，blocks: 512→512、512→512、512→256、256→256，输出Linear(256→7)\n正则化: BatchNorm1d、Dropout(p=0.3/0.3/0.2/0.2)、权重衰减=0.01\n损失函数: CrossEntropyLoss\n优化与调度: AdamW(lr=0.001, wd=0.01)、OneCycleLR(max_lr=0.01, total_steps=25*len(train_loader), pct_start=0.3)\n训练循环: 前向→损失→反向→更新→调度",
            "insights_from_parent": [
                "用TF-IDF和BERT特征训练LightGBM"
            ]
        },
        {
            "demo_id": "learning-agency-lab-automated-essay-scoring-2",
            "node_index": 14,
            "metric": 0.7705823586318292,
            "plan": "To improve the model's performance, we'll upgrade the text embedding model from\nBERT-base to DeBERTa-v3-base which demonstrates superior performance on NLP\ntasks through its disentangled attention mechanism. This change should capture\nricher semantic features while maintaining computational efficiency through\nadjusted batch sizing. We'll validate the improvement using quadratic weighted\nkappa.",
            "code": "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import cohen_kappa_score\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\nfrom lightgbm import LGBMRegressor\n\n# Load data\ntrain_df = pd.read_csv(\"./input/train.csv\")\ntest_df = pd.read_csv(\"./input/test.csv\")\n\n# Upgrade to DeBERTa-v3 with adjusted batch size\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\")\nmodel = AutoModel.from_pretrained(\"microsoft/deberta-v3-base\").to(\"cuda\")\n\n\ndef get_bert_embeddings(texts, batch_size=16):  # Reduced batch size for memory\n    model.eval()\n    embeddings = []\n    for i in range(0, len(texts), batch_size):\n        batch = texts[i : i + batch_size]\n        inputs = tokenizer(\n            batch, padding=True, truncation=True, max_length=512, return_tensors=\"pt\"\n        ).to(\"cuda\")\n        with torch.no_grad():\n            outputs = model(**inputs)\n        last_hidden = outputs.last_hidden_state\n        mask = inputs[\"attention_mask\"].unsqueeze(-1).expand(last_hidden.size()).float()\n        mean_embeddings = torch.sum(last_hidden * mask, 1) / torch.clamp(\n            mask.sum(1), min=1e-9\n        )\n        embeddings.append(mean_embeddings.cpu().numpy())\n    return np.concatenate(embeddings, axis=0)\n\n\nall_texts = pd.concat([train_df[\"full_text\"], test_df[\"full_text\"]]).tolist()\nall_bert = get_bert_embeddings(all_texts)\ntrain_bert = all_bert[: len(train_df)]\ntest_bert = all_bert[len(train_df) :]\n\n# Split training data and get indices\nX_train_part, X_val_part, y_train_part, y_val_part = train_test_split(\n    train_df[\"full_text\"], train_df[\"score\"], test_size=0.2, random_state=42\n)\ntrain_indices = X_train_part.index\nval_indices = X_val_part.index\n\n# Validation phase TF-IDF\ntfidf_part = TfidfVectorizer(max_features=10000)\nX_train_tfidf = tfidf_part.fit_transform(X_train_part)\nX_val_tfidf = tfidf_part.transform(X_val_part)\n\n# Combine features for validation\nX_train_combined = np.hstack([X_train_tfidf.toarray(), train_bert[train_indices]])\nX_val_combined = np.hstack([X_val_tfidf.toarray(), train_bert[val_indices]])\n\n# Train and validate\nlgb = LGBMRegressor(n_estimators=500, random_state=42)\nlgb.fit(X_train_combined, y_train_part)\nval_preds = lgb.predict(X_val_combined)\nval_preds = np.clip(np.round(val_preds), 1, 6).astype(int)\nkappa = cohen_kappa_score(y_val_part, val_preds, weights=\"quadratic\")\nprint(f\"Validation Quadratic Weighted Kappa: {kappa}\")\n\n# Full training\ntfidf_full = TfidfVectorizer(max_features=10000)\nX_full_tfidf = tfidf_full.fit_transform(train_df[\"full_text\"])\nX_test_tfidf = tfidf_full.transform(test_df[\"full_text\"])\n\nX_full_combined = np.hstack([X_full_tfidf.toarray(), train_bert])\nX_test_combined = np.hstack([X_test_tfidf.toarray(), test_bert])\n\nlgb_full = LGBMRegressor(n_estimators=500, random_state=42)\nlgb_full.fit(X_full_combined, train_df[\"score\"])\ntest_preds = lgb_full.predict(X_test_combined)\ntest_preds = np.clip(np.round(test_preds), 1, 6).astype(int)\n\n# Create submission\npd.DataFrame({\"essay_id\": test_df[\"essay_id\"], \"score\": test_preds}).to_csv(\n    \"./submission/submission.csv\", index=False\n)\n",
            "analysis": "The code successfully executed and produced a submission file named 'submission.csv' in the './submission/' directory. The model achieved a validation quadratic weighted kappa score of 0.7706, indicating a good level of agreement between the predicted and actual essay scores. The execution log shows that the process completed without any errors or bugs.",
            "is_bug": "False",
            "parent_index": 13,
            "node_level_analysis": "数据划分: hold-out 80/20, random_state=42\n特征工程: StandardScaler 对10个数值特征 fit/transform（训练集fit，验证/测试transform）\n类别编码: one-hot→类别索引提取，Wilderness_Area1-4→Wilderness(0-3)、Soil_Type1-40→Soil(0-39)\n类别嵌入: Wilderness(4→4维)、Soil(40→16维)\n数据管道: TensorDataset + DataLoader batch=4096, shuffle=True(训练), num_workers=4；验证DataLoader batch=4096, no shuffle\n模型构建: Embedding + ResNet-MLP 含SE，input Linear(10+4+16→512)+BatchNorm1d+GELU，blocks: 512→512 (×2, Dropout p=0.3)、512→256 (Dropout p=0.2)、256→256 (Dropout p=0.2)，输出 Linear(256→7)\n正则化: BatchNorm1d、Dropout(p=0.3/0.2)、权重衰减=0.01\n损失函数: CrossEntropyLoss\n优化与调度: AdamW lr=0.001, wd=0.01；OneCycleLR max_lr=0.01, total_steps=25*len(train_loader), pct_start=0.3\n模型训练: 前向→损失→反向→更新→调度，epoch=25；基于验证集保存最优模型(\"./working/best_model.pt\")",
            "insights_from_parent": [
                "数据预处理：数据划分保持不变，嵌入推理批次减小以节省显存。",
                "特征工程：TF-IDF配置保持(max_features=10000)，与语义嵌入拼接使用。",
                "模型选择：将bert-base-uncased替换为microsoft/deberta-v3-base以提升语义表示。",
                "参数调整：embedding推理batch_size从32降到16以适配更大模型的显存需求。",
                "训练策略：验证指标与LightGBM流程不变，侧重评估更换模型带来的提升。"
            ]
        },
        {
            "demo_id": "learning-agency-lab-automated-essay-scoring-2",
            "node_index": 15,
            "metric": null,
            "plan": "To improve the model's performance, we'll fine-tune the DeBERTa model on the\nessay scoring task before extracting embeddings. This allows the model to learn\ndomain-specific representations that better capture scoring patterns. The fine-\ntuned embeddings are then combined with TF-IDF features and fed into LightGBM\nfor final prediction.",
            "code": "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import cohen_kappa_score\nfrom transformers import AutoTokenizer, AutoModel, Trainer, TrainingArguments\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom lightgbm import LGBMRegressor\nimport torch.nn as nn\n\n# Load data\ntrain_df = pd.read_csv(\"./input/train.csv\")\ntest_df = pd.read_csv(\"./input/test.csv\")\n\n\n# Custom dataset for fine-tuning\nclass EssayDataset(Dataset):\n    def __init__(self, texts, scores, tokenizer, max_length=512):\n        self.texts = texts\n        self.scores = scores\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        encoding = self.tokenizer(\n            text,\n            max_length=self.max_length,\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors=\"pt\",\n        )\n        return {\n            \"input_ids\": encoding[\"input_ids\"].flatten(),\n            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n            \"labels\": torch.tensor(self.scores[idx], dtype=torch.float),\n        }\n\n\n# Regression model with DeBERTa base\nclass DebertaRegressor(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.deberta = AutoModel.from_pretrained(\"microsoft/deberta-v3-base\")\n        self.regressor = nn.Linear(self.deberta.config.hidden_size, 1)\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.deberta(input_ids=input_ids, attention_mask=attention_mask)\n        pooled = outputs.last_hidden_state.mean(dim=1)\n        return self.regressor(pooled)\n\n\n# Prepare data splits\ntrain_texts, val_texts, train_scores, val_scores = train_test_split(\n    train_df[\"full_text\"], train_df[\"score\"], test_size=0.2, random_state=42\n)\n\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\")\ntrain_dataset = EssayDataset(train_texts.tolist(), train_scores.tolist(), tokenizer)\nval_dataset = EssayDataset(val_texts.tolist(), val_scores.tolist(), tokenizer)\n\n# Training configuration\nmodel = DebertaRegressor().to(\"cuda\")\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=3,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=16,\n    warmup_steps=500,\n    weight_decay=0.01,\n    logging_dir=\"./logs\",\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    fp16=True,\n    gradient_accumulation_steps=2,\n)\n\n\n# Custom trainer for regression\nclass RegressionTrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False):\n        labels = inputs.pop(\"labels\")\n        outputs = model(**inputs)\n        loss = nn.MSELoss()(outputs.squeeze(), labels)\n        return (loss, outputs) if return_outputs else loss\n\n\n# Train model\ntrainer = RegressionTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n)\ntrainer.train()\n\n# Save fine-tuned DeBERTa\nmodel.deberta.save_pretrained(\"./deberta_finetuned\")\n\n\n# Generate embeddings with fine-tuned model\ndef get_embeddings(texts, batch_size=16):\n    model = AutoModel.from_pretrained(\"./deberta_finetuned\").to(\"cuda\")\n    model.eval()\n    embeddings = []\n    for i in range(0, len(texts), batch_size):\n        batch = texts[i : i + batch_size]\n        inputs = tokenizer(\n            batch, padding=True, truncation=True, max_length=512, return_tensors=\"pt\"\n        ).to(\"cuda\")\n        with torch.no_grad():\n            outputs = model(**inputs)\n        embeddings.append(outputs.last_hidden_state.mean(dim=1).cpu().numpy())\n    return np.concatenate(embeddings, axis=0)\n\n\n# Generate all embeddings\nall_texts = pd.concat([train_df[\"full_text\"], test_df[\"full_text\"]]).tolist()\nall_bert = get_embeddings(all_texts)\ntrain_bert = all_bert[: len(train_df)]\ntest_bert = all_bert[len(train_df) :]\n\n# Feature engineering\ntfidf = TfidfVectorizer(max_features=10000)\nX_tfidf = tfidf.fit_transform(train_df[\"full_text\"])\nX_tfidf_test = tfidf.transform(test_df[\"full_text\"])\n\n# Combine features\nX_train = np.hstack([X_tfidf.toarray(), train_bert])\nX_test = np.hstack([X_tfidf_test.toarray(), test_bert])\n\n# Train final model\nlgb = LGBMRegressor(n_estimators=500, random_state=42)\nX_train_part, X_val_part, y_train, y_val = train_test_split(\n    X_train, train_df[\"score\"], test_size=0.2, random_state=42\n)\nlgb.fit(X_train_part, y_train)\n\n# Validate\nval_preds = np.clip(np.round(lgb.predict(X_val_part)), 1, 6).astype(int)\nkappa = cohen_kappa_score(y_val, val_preds, weights=\"quadratic\")\nprint(f\"Validation Quadratic Weighted Kappa: {kappa:.4f}\")\n\n# Final prediction\nlgb_full = LGBMRegressor(n_estimators=500, random_state=42)\nlgb_full.fit(X_train, train_df[\"score\"])\ntest_preds = np.clip(np.round(lgb_full.predict(X_test)), 1, 6).astype(int)\n\n# Save submission\npd.DataFrame({\"essay_id\": test_df[\"essay_id\"], \"score\": test_preds}).to_csv(\n    \"./submission/submission.csv\", index=False\n)\n",
            "analysis": "The execution of the code failed due to an error related to the WandB integration, specifically a missing API key configuration. As a result, the script could not proceed with the training phase, and no submission.csv file was produced. This indicates a bug in the setup of the WandB logging, preventing the script from running successfully.",
            "is_bug": "True",
            "parent_index": 14,
            "node_level_analysis": "数据划分: hold-out 80/20, random_state=42\n特征工程: one-hot→类别索引（Wilderness、Soil），数值特征标准化 StandardScaler（train fit, val/test transform）\n类别嵌入: Wilderness(4→4维)、Soil(40→16维)\n数据管道: TensorDataset + DataLoader，batch=4096，train shuffle=True，num_workers=4\n模型构建: Embedding + ResNet-MLP，输入10+4+16→512，残差块序列(512→512, 512→512, 512→256, 256→256)，输出256→7，含SE模块\n正则化: BatchNorm1d、Dropout(p=0.3/0.2)、权重衰减 wd=0.01\n损失函数: CrossEntropyLoss\n优化与调度: AdamW(lr=0.001, wd=0.01)，OneCycleLR(max_lr=0.01, total_steps=25*len(train_loader), pct_start=0.3)\n训练循环: 前向→损失→反向→更新→调度，epoch=25\n评估/模型选择: 按验证准确率保存最佳模型（best_model.pt）\n后处理/映射: 预测类别 +1 转回1-7，保存 submission.csv",
            "insights_from_parent": [
                "数据预处理：训练集划分用于监督微调，标签同时用于Transformer微调。",
                "特征工程：用微调后的DeBERTa生成语义嵌入，替代仅用预训练模型的嵌入。",
                "模型选择：新增DebertaRegressor进行任务微调，再以其嵌入+TF-IDF训练LightGBM。",
                "参数调整：引入Trainer超参（epochs=3、per_device_train_batch_size=8、fp16、grad_accumulation=2等）。",
                "训练策略：两阶段流程——先微调Transformer以学习领域特征，再提取嵌入训练LightGBM。"
            ]
        },
        {
            "demo_id": "learning-agency-lab-automated-essay-scoring-2",
            "node_index": 16,
            "metric": null,
            "plan": "To address the WandB error and ensure submission generation, we disable logging\nintegrations and verify file paths. The key fix is adding `report_to=\"none\"` to\nTrainingArguments to prevent external API calls. We also ensure proper model\nreloading and validate the submission file path.",
            "code": "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import cohen_kappa_score\nfrom transformers import AutoTokenizer, AutoModel, Trainer, TrainingArguments\nimport torch\nfrom torch.utils.data import Dataset\nfrom lightgbm import LGBMRegressor\nimport torch.nn as nn\n\n# Load data\ntrain_df = pd.read_csv(\"./input/train.csv\")\ntest_df = pd.read_csv(\"./input/test.csv\")\n\n\n# Custom dataset for fine-tuning\nclass EssayDataset(Dataset):\n    def __init__(self, texts, scores, tokenizer, max_length=512):\n        self.texts = texts\n        self.scores = scores\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        encoding = self.tokenizer(\n            text,\n            max_length=self.max_length,\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors=\"pt\",\n        )\n        return {\n            \"input_ids\": encoding[\"input_ids\"].flatten(),\n            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n            \"labels\": torch.tensor(self.scores[idx], dtype=torch.float),\n        }\n\n\n# Regression model with DeBERTa base\nclass DebertaRegressor(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.deberta = AutoModel.from_pretrained(\"microsoft/deberta-v3-base\")\n        self.regressor = nn.Linear(self.deberta.config.hidden_size, 1)\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.deberta(input_ids=input_ids, attention_mask=attention_mask)\n        pooled = outputs.last_hidden_state.mean(dim=1)\n        return self.regressor(pooled)\n\n\n# Prepare data splits\ntrain_texts, val_texts, train_scores, val_scores = train_test_split(\n    train_df[\"full_text\"], train_df[\"score\"], test_size=0.2, random_state=42\n)\n\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\")\ntrain_dataset = EssayDataset(train_texts.tolist(), train_scores.tolist(), tokenizer)\nval_dataset = EssayDataset(val_texts.tolist(), val_scores.tolist(), tokenizer)\n\n# Training configuration with disabled logging\nmodel = DebertaRegressor().to(\"cuda\")\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=3,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=16,\n    warmup_steps=500,\n    weight_decay=0.01,\n    logging_dir=\"./logs\",\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    fp16=True,\n    gradient_accumulation_steps=2,\n    report_to=\"none\",  # Disable external logging\n)\n\n\n# Custom trainer for regression\nclass RegressionTrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False):\n        labels = inputs.pop(\"labels\")\n        outputs = model(**inputs)\n        loss = nn.MSELoss()(outputs.squeeze(), labels)\n        return (loss, outputs) if return_outputs else loss\n\n\n# Train model\ntrainer = RegressionTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n)\ntrainer.train()\n\n\n# Generate embeddings with fine-tuned model\ndef get_embeddings(texts, batch_size=16):\n    model = AutoModel.from_pretrained(\"./results/checkpoint-best\").to(\"cuda\")\n    model.eval()\n    embeddings = []\n    for i in range(0, len(texts), batch_size):\n        batch = texts[i : i + batch_size]\n        inputs = tokenizer(\n            batch, padding=True, truncation=True, max_length=512, return_tensors=\"pt\"\n        ).to(\"cuda\")\n        with torch.no_grad():\n            outputs = model(**inputs)\n        embeddings.append(outputs.last_hidden_state.mean(dim=1).cpu().numpy())\n    return np.concatenate(embeddings, axis=0)\n\n\n# Generate all embeddings\nall_texts = pd.concat([train_df[\"full_text\"], test_df[\"full_text\"]]).tolist()\nall_bert = get_embeddings(all_texts)\ntrain_bert = all_bert[: len(train_df)]\ntest_bert = all_bert[len(train_df) :]\n\n# Feature engineering\ntfidf = TfidfVectorizer(max_features=10000)\nX_tfidf = tfidf.fit_transform(train_df[\"full_text\"])\nX_tfidf_test = tfidf.transform(test_df[\"full_text\"])\n\n# Combine features\nX_train = np.hstack([X_tfidf.toarray(), train_bert])\nX_test = np.hstack([X_tfidf_test.toarray(), test_bert])\n\n# Train final model\nlgb = LGBMRegressor(n_estimators=500, random_state=42)\nX_train_part, X_val_part, y_train, y_val = train_test_split(\n    X_train, train_df[\"score\"], test_size=0.2, random_state=42\n)\nlgb.fit(X_train_part, y_train)\n\n# Validate\nval_preds = np.clip(np.round(lgb.predict(X_val_part)), 1, 6).astype(int)\nkappa = cohen_kappa_score(y_val, val_preds, weights=\"quadratic\")\nprint(f\"Validation Quadratic Weighted Kappa: {kappa:.4f}\")\n\n# Final prediction\nlgb_full = LGBMRegressor(n_estimators=500, random_state=42)\nlgb_full.fit(X_train, train_df[\"score\"])\ntest_preds = np.clip(np.round(lgb_full.predict(X_test)), 1, 6).astype(int)\n\n# Save submission\nsubmission_df = pd.DataFrame({\"essay_id\": test_df[\"essay_id\"], \"score\": test_preds})\nsubmission_df.to_csv(\"./submission/submission.csv\", index=False)\n",
            "analysis": "The execution of the code failed due to a TypeError in the custom RegressionTrainer class. The error occurred because the compute_loss method was called with an unexpected keyword argument 'num_items_in_batch'. As a result, the training process could not be completed, and no validation metric was obtained. However, the code does attempt to save predictions to a 'submission.csv' file in the './submission/' directory.",
            "is_bug": "True",
            "parent_index": 15,
            "node_level_analysis": "数据划分: hold-out 80/20，random_state=42\n特征工程: 数值标准化(StandardScaler) 对10个数值列 fit_transform/transform\n类别编码: 从one-hot恢复索引（idxmax→提取数字→减1）\n类别嵌入: Wilderness(4→4维)、Soil(40→16维)\n数据管道: TensorDataset + DataLoader(batch=4096, shuffle=True（train）, num_workers=4)\n模型构建: Embedding + ResNet-MLP + SE，输入10+4+16→512，残差块序列512→512、512→512、512→256、256→256，输出256→7\n正则化: BatchNorm1d、Dropout(p=0.3/0.2)、权重衰减=0.01\n损失函数: CrossEntropyLoss\n优化与调度: AdamW(lr=0.001, wd=0.01)、OneCycleLR(max_lr=0.01, total_steps=25*len(train_loader), pct_start=0.3)\n训练循环: epoch=25，前向→损失→反向→更新→调度，按epoch验证并保存best_model.pt",
            "insights_from_parent": [
                "数据预处理：无本质变化，但确保提交路径与文件夹存在以生成输出。",
                "特征工程：嵌入提取改为从训练输出目录('./results/checkpoint-best')加载模型生成。",
                "模型选择：仍为微调DeBERTa + LightGBM，但加入对外部日志禁用以避免异常。",
                "参数调整：在TrainingArguments中添加report_to='none'以禁用WandB等外部集成。",
                "训练策略：修复外部日志问题，确保最佳checkpoint能正确保存并被后续嵌入提取使用。"
            ]
        },
        {
            "demo_id": "learning-agency-lab-automated-essay-scoring-2",
            "node_index": 17,
            "metric": null,
            "plan": "To fix the previous errors, we need to adjust two main components: 1) Fix the\nTrainer's loss computation by adding **kwargs parameter to handle unexpected\narguments, and 2) Correctly load the fine-tuned model to generate embeddings.\nAdditionally, we'll simplify the embedding extraction process by using mean\npooling from the fine-tuned model.",
            "code": "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import cohen_kappa_score\nfrom transformers import AutoTokenizer, AutoModel, Trainer, TrainingArguments\nimport torch\nfrom torch.utils.data import Dataset\nfrom lightgbm import LGBMRegressor\nimport torch.nn as nn\n\n# Load data\ntrain_df = pd.read_csv(\"./input/train.csv\")\ntest_df = pd.read_csv(\"./input/test.csv\")\n\n\n# Custom dataset for fine-tuning\nclass EssayDataset(Dataset):\n    def __init__(self, texts, scores, tokenizer, max_length=512):\n        self.texts = texts\n        self.scores = scores\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        encoding = self.tokenizer(\n            text,\n            max_length=self.max_length,\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors=\"pt\",\n        )\n        return {\n            \"input_ids\": encoding[\"input_ids\"].flatten(),\n            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n            \"labels\": torch.tensor(self.scores[idx], dtype=torch.float),\n        }\n\n\n# Regression model with DeBERTa base\nclass DebertaRegressor(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.deberta = AutoModel.from_pretrained(\"microsoft/deberta-v3-base\")\n        self.regressor = nn.Linear(self.deberta.config.hidden_size, 1)\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.deberta(input_ids=input_ids, attention_mask=attention_mask)\n        pooled = outputs.last_hidden_state.mean(dim=1)\n        return self.regressor(pooled)\n\n\n# Prepare data splits\ntrain_texts, val_texts, train_scores, val_scores = train_test_split(\n    train_df[\"full_text\"], train_df[\"score\"], test_size=0.2, random_state=42\n)\n\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\")\ntrain_dataset = EssayDataset(train_texts.tolist(), train_scores.tolist(), tokenizer)\nval_dataset = EssayDataset(val_texts.tolist(), val_scores.tolist(), tokenizer)\n\n# Training configuration\nmodel = DebertaRegressor().to(\"cuda\")\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=3,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=16,\n    warmup_steps=500,\n    weight_decay=0.01,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    fp16=True,\n    gradient_accumulation_steps=2,\n    report_to=\"none\",\n)\n\n\n# Fixed Trainer class\nclass RegressionTrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n        labels = inputs.pop(\"labels\")\n        outputs = model(**inputs)\n        loss = nn.MSELoss()(outputs.squeeze(), labels)\n        return (loss, outputs) if return_outputs else loss\n\n\n# Train model\ntrainer = RegressionTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n)\ntrainer.train()\n\n\n# Fixed embedding generation\ndef get_embeddings(texts, batch_size=16):\n    model = DebertaRegressor()\n    model.load_state_dict(torch.load(\"./results/checkpoint-best/pytorch_model.bin\"))\n    model = model.to(\"cuda\").eval()\n    embeddings = []\n    for i in range(0, len(texts), batch_size):\n        batch = texts[i : i + batch_size]\n        inputs = tokenizer(\n            batch, padding=True, truncation=True, max_length=512, return_tensors=\"pt\"\n        ).to(\"cuda\")\n        with torch.no_grad():\n            outputs = model.deberta(**inputs)\n        embeddings.append(outputs.last_hidden_state.mean(dim=1).cpu().numpy())\n    return np.concatenate(embeddings, axis=0)\n\n\n# Generate embeddings\nall_texts = pd.concat([train_df[\"full_text\"], test_df[\"full_text\"]]).tolist()\nall_bert = get_embeddings(all_texts)\ntrain_bert = all_bert[: len(train_df)]\ntest_bert = all_bert[len(train_df) :]\n\n# Feature engineering\ntfidf = TfidfVectorizer(max_features=10000)\nX_tfidf = tfidf.fit_transform(train_df[\"full_text\"])\nX_tfidf_test = tfidf.transform(test_df[\"full_text\"])\n\n# Combine features\nX_train = np.hstack([X_tfidf.toarray(), train_bert])\nX_test = np.hstack([X_tfidf_test.toarray(), test_bert])\n\n# Train final model\nlgb = LGBMRegressor(n_estimators=500, random_state=42)\nX_train_part, X_val_part, y_train, y_val = train_test_split(\n    X_train, train_df[\"score\"], test_size=0.2, random_state=42\n)\nlgb.fit(X_train_part, y_train)\n\n# Validate\nval_preds = np.clip(np.round(lgb.predict(X_val_part)), 1, 6).astype(int)\nkappa = cohen_kappa_score(y_val, val_preds, weights=\"quadratic\")\nprint(f\"Validation Quadratic Weighted Kappa: {kappa:.4f}\")\n\n# Final prediction\nlgb_full = LGBMRegressor(n_estimators=500, random_state=42)\nlgb_full.fit(X_train, train_df[\"score\"])\ntest_preds = np.clip(np.round(lgb_full.predict(X_test)), 1, 6).astype(int)\n\n# Save submission\nsubmission_df = pd.DataFrame({\"essay_id\": test_df[\"essay_id\"], \"score\": test_preds})\nsubmission_df.to_csv(\"./submission/submission.csv\", index=False)\n",
            "analysis": "The code execution failed due to a KeyError related to the 'labels' key in the compute_loss function. This indicates that the input data did not contain the expected 'labels' field, which is necessary for calculating the loss during training. As a result, the script was unable to produce the required submission.csv file.",
            "is_bug": "True",
            "parent_index": 16,
            "node_level_analysis": "数据划分: hold-out 80/20, random_state=42\n特征工程: 从 one-hot 提取分类索引（Wilderness, Soil），数值特征标准化 StandardScaler（10个特征）\n类别嵌入: Wilderness(4→4维)、Soil(40→16维)\n数据管道: 张量化(torch.tensor dtype=float32)+DataLoader(batch=4096, shuffle=True, num_workers=4)\n模型构建: Embedding+ResNet-MLP（输入10+4+16→512，blocks: 512→512×2（Dropout p=0.3）、512→256（p=0.2）、256→256（p=0.2），输出256→7），ResidualBlock含SE(reduction=16)\n正则化: BatchNorm1d、Dropout(p=0.3/0.2)、权重衰减=0.01\n损失函数: CrossEntropyLoss\n优化与调度: AdamW(lr=0.001, wd=0.01) + OneCycleLR(max_lr=0.01, total_steps=25*len(train_loader), pct_start=0.3), epoch=25\n模型训练: 前向→损失→反向→更新→调度",
            "insights_from_parent": [
                "数据预处理：保持原有划分与标签使用方式不变。",
                "特征工程：修正嵌入加载逻辑，使用自定义DebertaRegressor加载state_dict并做mean pooling。",
                "模型选择：继续使用自定义微调模型生成嵌入，随后用LightGBM完成回归预测。",
                "参数调整：修复Trainer.compute_loss签名(add **kwargs)以兼容HF传入的额外参数，并改为加载pytorch_model.bin。",
                "训练策略：提高训练兼容性与模型重载可靠性，确保可重复的嵌入生成流程。"
            ]
        },
        {
            "demo_id": "learning-agency-lab-automated-essay-scoring-2",
            "node_index": 18,
            "metric": null,
            "plan": "The KeyError occurs because the custom Trainer's compute_loss function expects\n'labels' in the inputs, but the data pipeline doesn't provide them correctly. To\nfix this, we'll use AutoModelForSequenceClassification with regression output\nand leverage Hugging Face's built-in loss handling. We'll also ensure proper\nmodel loading for embeddings and submission file creation.",
            "code": "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import cohen_kappa_score\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n    Trainer,\n    TrainingArguments,\n)\nimport torch\nfrom torch.utils.data import Dataset\nfrom lightgbm import LGBMRegressor\nimport os\n\n# Ensure submission directory exists\nos.makedirs(\"./submission\", exist_ok=True)\n\n# Load data\ntrain_df = pd.read_csv(\"./input/train.csv\")\ntest_df = pd.read_csv(\"./input/test.csv\")\n\n\n# Custom dataset\nclass EssayDataset(Dataset):\n    def __init__(self, texts, scores, tokenizer, max_length=512):\n        self.texts = texts\n        self.scores = scores\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        encoding = self.tokenizer(\n            text,\n            max_length=self.max_length,\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors=\"pt\",\n        )\n        return {\n            \"input_ids\": encoding[\"input_ids\"].flatten(),\n            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n            \"labels\": torch.tensor(self.scores[idx], dtype=torch.float),\n        }\n\n\n# Prepare data splits\ntrain_texts, val_texts, train_scores, val_scores = train_test_split(\n    train_df[\"full_text\"], train_df[\"score\"], test_size=0.2, random_state=42\n)\n\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\")\ntrain_dataset = EssayDataset(train_texts.tolist(), train_scores.tolist(), tokenizer)\nval_dataset = EssayDataset(val_texts.tolist(), val_scores.tolist(), tokenizer)\n\n# Configure model and training\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"microsoft/deberta-v3-base\", num_labels=1\n).to(\"cuda\")\n\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=3,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=16,\n    warmup_steps=500,\n    weight_decay=0.01,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    fp16=True,\n    gradient_accumulation_steps=2,\n    report_to=\"none\",\n    problem_type=\"regression\",\n)\n\n# Train model\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n)\ntrainer.train()\n\n\n# Embedding generation\ndef get_embeddings(texts, batch_size=16):\n    model = AutoModelForSequenceClassification.from_pretrained(\n        \"./results/checkpoint-best\"\n    ).to(\"cuda\")\n    model.eval()\n    embeddings = []\n    for i in range(0, len(texts), batch_size):\n        batch = texts[i : i + batch_size]\n        inputs = tokenizer(\n            batch, padding=True, truncation=True, max_length=512, return_tensors=\"pt\"\n        ).to(\"cuda\")\n        with torch.no_grad():\n            outputs = model.deberta(**inputs)\n        embeddings.append(outputs.last_hidden_state.mean(dim=1).cpu().numpy())\n    return np.concatenate(embeddings, axis=0)\n\n\n# Generate features\nall_texts = pd.concat([train_df[\"full_text\"], test_df[\"full_text\"]]).tolist()\nall_bert = get_embeddings(all_texts)\ntrain_bert = all_bert[: len(train_df)]\ntest_bert = all_bert[len(train_df) :]\n\ntfidf = TfidfVectorizer(max_features=10000)\nX_tfidf = tfidf.fit_transform(train_df[\"full_text\"])\nX_tfidf_test = tfidf.transform(test_df[\"full_text\"])\n\nX_train = np.hstack([X_tfidf.toarray(), train_bert])\nX_test = np.hstack([X_tfidf_test.toarray(), test_bert])\n\n# Train final model and validate\nlgb = LGBMRegressor(n_estimators=500, random_state=42)\nX_train_part, X_val_part, y_train, y_val = train_test_split(\n    X_train, train_df[\"score\"], test_size=0.2, random_state=42\n)\nlgb.fit(X_train_part, y_train)\n\nval_preds = np.clip(np.round(lgb.predict(X_val_part)), 1, 6).astype(int)\nkappa = cohen_kappa_score(y_val, val_preds, weights=\"quadratic\")\nprint(f\"Validation Quadratic Weighted Kappa: {kappa:.4f}\")\n\n# Generate submission\nlgb_full = LGBMRegressor(n_estimators=500, random_state=42)\nlgb_full.fit(X_train, train_df[\"score\"])\ntest_preds = np.clip(np.round(lgb_full.predict(X_test)), 1, 6).astype(int)\n\nsubmission_df = pd.DataFrame({\"essay_id\": test_df[\"essay_id\"], \"score\": test_preds})\nsubmission_df.to_csv(\"./submission/submission.csv\", index=False)\n",
            "analysis": "The code execution failed due to a TypeError caused by an unexpected keyword argument 'problem_type' in the TrainingArguments initialization. As a result, the model training did not complete, and no validation metric was calculated. The output log indicates that a submission.csv file was intended to be created, but the error prevented successful completion of the script.",
            "is_bug": "True",
            "parent_index": 17,
            "node_level_analysis": "数据划分: hold-out 80/20, random_state=42\n特征工程: 数值标准化 StandardScaler（10个数值特征）\n类别编码: one-hot→index via idxmax then -1（Wilderness, Soil）\n类别嵌入: Wilderness(4→4维)、Soil(40→16维)\n数据管道: 张量化+DataLoader（batch=4096, shuffle=True, num_workers=4）\n模型构建: EmbeddingResNet；input 10+4+16→512，blocks 512→512、512→512、512→256、256→256，out 256→7，ResidualBlock含SE\n正则化: BatchNorm1d、Dropout(p=0.3/0.2)、weight_decay=0.01\n损失函数: CrossEntropyLoss\n优化与调度: AdamW(lr=0.001, wd=0.01)、OneCycleLR(max_lr=0.01, total_steps=25*len(train_loader), pct_start=0.3)\n模型训练: 前向→损失→反向→更新→调度",
            "insights_from_parent": [
                "数据预处理：确保提交目录存在并维持一致的数据划分和预处理步骤。",
                "特征工程：改用AutoModelForSequenceClassification微调后取deberta输出mean pooling作为嵌入。",
                "模型选择：从自定义Regressor切换到AutoModelForSequenceClassification(num_labels=1)并声明回归任务。",
                "参数调整：利用HF内置loss与problem_type='regression'避免自定义loss引发的KeyError，保留report_to='none'。",
                "训练策略：简化与稳健化训练流程，依赖Hugging Face的内置回归处理以减少自定义Trainer的错误，确保能正确生成提交文件。"
            ]
        }
    ],
    [
        {
            "demo_id": "learning-agency-lab-automated-essay-scoring-2",
            "node_index": 19,
            "metric": 0.7866,
            "plan": "To address the automated essay scoring challenge, we'll leverage the Longformer\nmodel's ability to process lengthy texts and employ ordinal regression using the\nCORAL method. This approach captures document-level context efficiently while\nrespecting the ordered nature of scoring. The model will be fine-tuned with a\ncustom loss function that treats the 1-6 score prediction as an ordinal\nclassification task.",
            "code": "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import cohen_kappa_score\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer, AutoModel, AdamW\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# Load data\ntrain_df = pd.read_csv(\"./input/train.csv\")\ntest_df = pd.read_csv(\"./input/test.csv\")\n\n# Split into train and validation\ntrain_df, val_df = train_test_split(\n    train_df, test_size=0.2, random_state=42, stratify=train_df[\"score\"]\n)\n\n\n# Dataset and tokenizer\nclass EssayDataset(Dataset):\n    def __init__(self, texts, scores, tokenizer, max_length=4096):\n        self.texts = texts\n        self.scores = scores\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        score = self.scores[idx] - 1  # Convert to 0-based\n        encoding = self.tokenizer.encode_plus(\n            text,\n            max_length=self.max_length,\n            padding=\"max_length\",\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors=\"pt\",\n        )\n        return {\n            \"input_ids\": encoding[\"input_ids\"].flatten(),\n            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n            \"score\": torch.tensor(score, dtype=torch.long),\n        }\n\n\ntokenizer = AutoTokenizer.from_pretrained(\"allenai/longformer-base-4096\")\ntrain_dataset = EssayDataset(\n    train_df[\"full_text\"].values, train_df[\"score\"].values, tokenizer\n)\nval_dataset = EssayDataset(\n    val_df[\"full_text\"].values, val_df[\"score\"].values, tokenizer\n)\ntest_dataset = EssayDataset(\n    test_df[\"full_text\"].values, np.zeros(len(test_df)), tokenizer\n)\n\n# Data loaders\nbatch_size = 8\ntrain_loader = DataLoader(\n    train_dataset, batch_size=batch_size, shuffle=True, num_workers=4\n)\nval_loader = DataLoader(\n    val_dataset, batch_size=batch_size, shuffle=False, num_workers=4\n)\ntest_loader = DataLoader(\n    test_dataset, batch_size=batch_size, shuffle=False, num_workers=4\n)\n\n\n# Model with CORAL ordinal regression\nclass CoralLongformer(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.base = AutoModel.from_pretrained(\"allenai/longformer-base-4096\")\n        self.dropout = nn.Dropout(0.1)\n        self.classifier = nn.Linear(\n            self.base.config.hidden_size, 5\n        )  # 6 classes -> 5 thresholds\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.base(input_ids, attention_mask=attention_mask)\n        pooled = outputs.last_hidden_state[:, 0, :]\n        return self.classifier(self.dropout(pooled))\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = CoralLongformer().to(device)\noptimizer = AdamW(model.parameters(), lr=2e-5)\n\n\n# Training loop\ndef coral_loss(logits, targets):\n    losses = [\n        F.binary_cross_entropy_with_logits(logits[:, i], (targets > i).float())\n        for i in range(logits.size(1))\n    ]\n    return sum(losses)\n\n\nfor epoch in range(3):\n    model.train()\n    for batch in train_loader:\n        optimizer.zero_grad()\n        inputs = {k: v.to(device) for k, v in batch.items() if k != \"score\"}\n        logits = model(**inputs)\n        loss = coral_loss(logits, batch[\"score\"].to(device))\n        loss.backward()\n        optimizer.step()\n\n# Validation\nmodel.eval()\nval_preds, val_true = [], []\nwith torch.no_grad():\n    for batch in val_loader:\n        inputs = {k: v.to(device) for k, v in batch.items() if k != \"score\"}\n        logits = model(**inputs)\n        preds = (torch.sigmoid(logits) > 0.5).sum(dim=1) + 1\n        val_preds.extend(preds.cpu().numpy())\n        val_true.extend((batch[\"score\"] + 1).numpy())\n\nkappa = cohen_kappa_score(val_true, val_preds, weights=\"quadratic\")\nprint(f\"Validation Quadratic Weighted Kappa: {kappa:.4f}\")\n\n# Test predictions\nmodel.eval()\ntest_preds = []\nwith torch.no_grad():\n    for batch in test_loader:\n        inputs = {k: v.to(device) for k, v in batch.items() if k != \"score\"}\n        logits = model(**inputs)\n        preds = (torch.sigmoid(logits) > 0.5).sum(dim=1) + 1\n        test_preds.extend(preds.cpu().numpy())\n\npd.DataFrame({\"essay_id\": test_df[\"essay_id\"], \"score\": test_preds}).to_csv(\n    \"./submission/submission.csv\", index=False\n)\n",
            "analysis": "The code successfully trained a model to predict essay scores and saved the predictions in a 'submission.csv' file in the './submission/' directory. The validation phase showed a Quadratic Weighted Kappa score of 0.7866, indicating a good level of agreement between the predicted and actual scores. The metric used should be maximized, as a higher Kappa score represents better performance.",
            "is_bug": "False",
            "parent_index": 0,
            "node_level_analysis": "数据划分: hold-out 80/20 (test_size=0.2), random_state=42\n特征工程: 数值标准化(StandardScaler) 对10个数值特征\n类别编码: one-hot→索引 (Wilderness_Area1-4→0-3, Soil_Type1-40→0-39)\n类别嵌入: Wilderness(4→4维)、Soil(40→16维)\n数据管道: 张量化(TensorDataset)、DataLoader batch=4096、shuffle(train=True)、num_workers=4\n模型构建: Embedding + ResNet-MLP，输入10+4+16→512，残差块(Linear→BatchNorm1d→GELU→Linear→BatchNorm1d)+SE，最终线性输出7类\n正则化: BatchNorm1d、Dropout(p=0.3/0.2)、权重衰减 wd=0.01\n损失函数: CrossEntropyLoss\n优化与调度: AdamW lr=0.001, wd=0.01；OneCycleLR max_lr=0.01, total_steps=25*len(train_loader), pct_start=0.3\n模型训练: 前向→损失→反向→更新→调度, epoch=25",
            "insights_from_parent": [
                "使用Longformer与CORAL序回归"
            ]
        },
        {
            "demo_id": "learning-agency-lab-automated-essay-scoring-2",
            "node_index": 20,
            "metric": 0.813,
            "plan": "To improve the model's ability to capture contextual information from long\nessays, we'll replace the CLS token pooling with mean pooling over all token\nembeddings. This better aggregates information from the full essay text and may\nbetter preserve semantic nuances important for scoring. The modified pooling\nlayer will be implemented with masked averaging to ignore padding tokens.",
            "code": "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import cohen_kappa_score\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer, AutoModel, AdamW\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# Load data\ntrain_df = pd.read_csv(\"./input/train.csv\")\ntest_df = pd.read_csv(\"./input/test.csv\")\n\n# Split into train and validation\ntrain_df, val_df = train_test_split(\n    train_df, test_size=0.2, random_state=42, stratify=train_df[\"score\"]\n)\n\n\n# Dataset and tokenizer\nclass EssayDataset(Dataset):\n    def __init__(self, texts, scores, tokenizer, max_length=4096):\n        self.texts = texts\n        self.scores = scores\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        score = self.scores[idx] - 1  # Convert to 0-based\n        encoding = self.tokenizer.encode_plus(\n            text,\n            max_length=self.max_length,\n            padding=\"max_length\",\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors=\"pt\",\n        )\n        return {\n            \"input_ids\": encoding[\"input_ids\"].flatten(),\n            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n            \"score\": torch.tensor(score, dtype=torch.long),\n        }\n\n\ntokenizer = AutoTokenizer.from_pretrained(\"allenai/longformer-base-4096\")\ntrain_dataset = EssayDataset(\n    train_df[\"full_text\"].values, train_df[\"score\"].values, tokenizer\n)\nval_dataset = EssayDataset(\n    val_df[\"full_text\"].values, val_df[\"score\"].values, tokenizer\n)\ntest_dataset = EssayDataset(\n    test_df[\"full_text\"].values, np.zeros(len(test_df)), tokenizer\n)\n\n# Data loaders\nbatch_size = 8\ntrain_loader = DataLoader(\n    train_dataset, batch_size=batch_size, shuffle=True, num_workers=4\n)\nval_loader = DataLoader(\n    val_dataset, batch_size=batch_size, shuffle=False, num_workers=4\n)\ntest_loader = DataLoader(\n    test_dataset, batch_size=batch_size, shuffle=False, num_workers=4\n)\n\n\n# Modified model with mean pooling\nclass CoralLongformer(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.base = AutoModel.from_pretrained(\"allenai/longformer-base-4096\")\n        self.dropout = nn.Dropout(0.2)\n        self.classifier = nn.Linear(self.base.config.hidden_size, 5)\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.base(input_ids, attention_mask=attention_mask)\n        hidden_states = outputs.last_hidden_state\n\n        # Mean pooling with attention mask\n        input_mask_expanded = (\n            attention_mask.unsqueeze(-1).expand(hidden_states.size()).float()\n        )\n        sum_embeddings = torch.sum(hidden_states * input_mask_expanded, 1)\n        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n        pooled = sum_embeddings / sum_mask\n\n        return self.classifier(self.dropout(pooled))\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = CoralLongformer().to(device)\noptimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n\n\n# Training loop with learning rate warmup\ndef coral_loss(logits, targets):\n    losses = [\n        F.binary_cross_entropy_with_logits(logits[:, i], (targets > i).float())\n        for i in range(logits.size(1))\n    ]\n    return sum(losses)\n\n\ntotal_steps = len(train_loader) * 4\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer, max_lr=2e-5, total_steps=total_steps\n)\n\nfor epoch in range(4):\n    model.train()\n    for batch in train_loader:\n        optimizer.zero_grad()\n        inputs = {k: v.to(device) for k, v in batch.items() if k != \"score\"}\n        logits = model(**inputs)\n        loss = coral_loss(logits, batch[\"score\"].to(device))\n        loss.backward()\n        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        scheduler.step()\n\n# Validation\nmodel.eval()\nval_preds, val_true = [], []\nwith torch.no_grad():\n    for batch in val_loader:\n        inputs = {k: v.to(device) for k, v in batch.items() if k != \"score\"}\n        logits = model(**inputs)\n        preds = (torch.sigmoid(logits) > 0.5).sum(dim=1) + 1\n        val_preds.extend(preds.cpu().numpy())\n        val_true.extend((batch[\"score\"] + 1).numpy())\n\nkappa = cohen_kappa_score(val_true, val_preds, weights=\"quadratic\")\nprint(f\"Validation Quadratic Weighted Kappa: {kappa:.4f}\")\n\n# Test predictions\nmodel.eval()\ntest_preds = []\nwith torch.no_grad():\n    for batch in test_loader:\n        inputs = {k: v.to(device) for k, v in batch.items() if k != \"score\"}\n        logits = model(**inputs)\n        preds = (torch.sigmoid(logits) > 0.5).sum(dim=1) + 1\n        test_preds.extend(preds.cpu().numpy())\n\npd.DataFrame({\"essay_id\": test_df[\"essay_id\"], \"score\": test_preds}).to_csv(\n    \"./submission/submission.csv\", index=False\n)\n",
            "analysis": "The model successfully trained and evaluated on the validation set, achieving a Quadratic Weighted Kappa score of 0.8130. This indicates a high level of agreement between the predicted and actual essay scores. The predictions for the test set were saved in the required 'submission.csv' file, indicating that the submission process was correctly implemented.",
            "is_bug": "False",
            "parent_index": 19,
            "node_level_analysis": "数据划分: hold-out 80%/20%, random_state=42\n特征工程: 数值特征标准化(StandardScaler)对10个数值列\n类别编码: one-hot→索引(idxmax - 1)用于 Wilderness、Soil\n类别嵌入: Wilderness(4→4维)、Soil(40→16维)\n数据管道: 张量化(TensorDataset)、DataLoader batch=4096、shuffle(train=True)、num_workers=4\n模型构建: Embedding + ResNet-MLP + SE，输入 Linear(10+4+16→512)、blocks: 512→512、512→512、512→256、256→256、输出 Linear(256→7)，ResidualBlock含shortcut线性变换、BatchNorm1d、GELU、SE(out_dim//16)\n正则化: BatchNorm1d、Dropout(p=0.3/0.3/0.2/0.2)、权重衰减 wd=0.01\n损失函数: CrossEntropyLoss\n优化与调度: AdamW(lr=0.001, wd=0.01)、OneCycleLR(max_lr=0.01, total_steps=25*len(train_loader), pct_start=0.3)\n训练循环: 前向→损失→反向→更新→调度",
            "insights_from_parent": [
                "数据预处理：保持不变，仍用4096截断与attention mask",
                "特征工程：无新增手工特征，改进体现在文本表示的池化方式",
                "模型选择：将CLS池化替换为masked mean pooling，更聚合全文信息",
                "参数调整：增大dropout(0.1→0.2)，优化器加入weight_decay=0.01",
                "训练策略：加入OneCycleLR调度、增加epoch(3→4)、添加梯度裁剪和scheduler步进"
            ]
        },
        {
            "demo_id": "learning-agency-lab-automated-essay-scoring-2",
            "node_index": 21,
            "metric": 0.8111,
            "plan": "To improve the model's ability to focus on relevant essay content, replace mean\npooling with an attention-based mechanism. This allows the model to dynamically\nweight important tokens when creating document representations. We'll add a\nlearned attention layer that computes token importance scores before pooling the\nhidden states.",
            "code": "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import cohen_kappa_score\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer, AutoModel, AdamW\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# Load data\ntrain_df = pd.read_csv(\"./input/train.csv\")\ntest_df = pd.read_csv(\"./input/test.csv\")\n\n# Split into train and validation\ntrain_df, val_df = train_test_split(\n    train_df, test_size=0.2, random_state=42, stratify=train_df[\"score\"]\n)\n\n\n# Dataset and tokenizer\nclass EssayDataset(Dataset):\n    def __init__(self, texts, scores, tokenizer, max_length=4096):\n        self.texts = texts\n        self.scores = scores\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        score = self.scores[idx] - 1  # Convert to 0-based\n        encoding = self.tokenizer.encode_plus(\n            text,\n            max_length=self.max_length,\n            padding=\"max_length\",\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors=\"pt\",\n        )\n        return {\n            \"input_ids\": encoding[\"input_ids\"].flatten(),\n            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n            \"score\": torch.tensor(score, dtype=torch.long),\n        }\n\n\ntokenizer = AutoTokenizer.from_pretrained(\"allenai/longformer-base-4096\")\ntrain_dataset = EssayDataset(\n    train_df[\"full_text\"].values, train_df[\"score\"].values, tokenizer\n)\nval_dataset = EssayDataset(\n    val_df[\"full_text\"].values, val_df[\"score\"].values, tokenizer\n)\ntest_dataset = EssayDataset(\n    test_df[\"full_text\"].values, np.zeros(len(test_df)), tokenizer\n)\n\n# Data loaders\nbatch_size = 8\ntrain_loader = DataLoader(\n    train_dataset, batch_size=batch_size, shuffle=True, num_workers=4\n)\nval_loader = DataLoader(\n    val_dataset, batch_size=batch_size, shuffle=False, num_workers=4\n)\ntest_loader = DataLoader(\n    test_dataset, batch_size=batch_size, shuffle=False, num_workers=4\n)\n\n\n# Model with attention pooling\nclass CoralLongformer(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.base = AutoModel.from_pretrained(\"allenai/longformer-base-4096\")\n        self.attention = nn.Sequential(\n            nn.Linear(self.base.config.hidden_size, 256),\n            nn.Tanh(),\n            nn.Linear(256, 1),\n            nn.Dropout(0.2),\n        )\n        self.classifier = nn.Linear(self.base.config.hidden_size, 5)\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.base(input_ids, attention_mask=attention_mask)\n        hidden_states = outputs.last_hidden_state\n\n        # Compute attention weights\n        att_scores = self.attention(hidden_states).squeeze(-1)\n        att_scores = att_scores.masked_fill(attention_mask == 0, -1e9)\n        att_weights = F.softmax(att_scores, dim=1)\n\n        # Weighted sum of hidden states\n        pooled = torch.sum(hidden_states * att_weights.unsqueeze(-1), dim=1)\n        return self.classifier(pooled)\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = CoralLongformer().to(device)\noptimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n\n\n# Training loop\ndef coral_loss(logits, targets):\n    losses = [\n        F.binary_cross_entropy_with_logits(logits[:, i], (targets > i).float())\n        for i in range(logits.size(1))\n    ]\n    return sum(losses)\n\n\ntotal_steps = len(train_loader) * 4\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer, max_lr=2e-5, total_steps=total_steps\n)\n\nfor epoch in range(4):\n    model.train()\n    for batch in train_loader:\n        optimizer.zero_grad()\n        inputs = {k: v.to(device) for k, v in batch.items() if k != \"score\"}\n        logits = model(**inputs)\n        loss = coral_loss(logits, batch[\"score\"].to(device))\n        loss.backward()\n        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        scheduler.step()\n\n# Validation\nmodel.eval()\nval_preds, val_true = [], []\nwith torch.no_grad():\n    for batch in val_loader:\n        inputs = {k: v.to(device) for k, v in batch.items() if k != \"score\"}\n        logits = model(**inputs)\n        preds = (torch.sigmoid(logits) > 0.5).sum(dim=1) + 1\n        val_preds.extend(preds.cpu().numpy())\n        val_true.extend((batch[\"score\"] + 1).numpy())\n\nkappa = cohen_kappa_score(val_true, val_preds, weights=\"quadratic\")\nprint(f\"Validation Quadratic Weighted Kappa: {kappa:.4f}\")\n\n# Test predictions\nmodel.eval()\ntest_preds = []\nwith torch.no_grad():\n    for batch in test_loader:\n        inputs = {k: v.to(device) for k, v in batch.items() if k != \"score\"}\n        logits = model(**inputs)\n        preds = (torch.sigmoid(logits) > 0.5).sum(dim=1) + 1\n        test_preds.extend(preds.cpu().numpy())\n\npd.DataFrame({\"essay_id\": test_df[\"essay_id\"], \"score\": test_preds}).to_csv(\n    \"./submission/submission.csv\", index=False\n)\n",
            "analysis": "The model successfully trained and evaluated on the validation set, achieving a strong Quadratic Weighted Kappa score of 0.8111, indicating good agreement with the true scores. The predictions for the test set were saved in the required 'submission.csv' format in the './submission/' directory. The execution completed without any errors within the time limit.",
            "is_bug": "False",
            "parent_index": 20,
            "node_level_analysis": "数据划分: hold-out 80/20, random_state=42  \n特征工程: 数值标准化(StandardScaler)对10个数值特征训练/验证/测试分别fit/transform/transform  \n类别编码: one-hot→类别索引(idxmax)得到 Wilderness(4类)、Soil(40类)  \n类别嵌入: Wilderness(4→4维)、Soil(40→16维)  \n数据管道: 张量化+DataLoader(batch=4096, shuffle=True(训练)、num_workers=4)  \n模型构建: Embedding + ResNet-风格MLP（输入10+4+16→512，多个残差块，SE模块），输出Linear→7类  \n正则化: BatchNorm、Dropout(p=0.3/0.2)、权重衰减=0.01  \n损失函数: 交叉熵 (CrossEntropyLoss)  \n优化与调度: AdamW(lr=0.001, wd=0.01) + OneCycleLR(max_lr=0.01, total_steps=25*len(train_loader), pct_start=0.3)  \n模型训练: 前向→损失→反向→更新→调度 (epoch=25)",
            "insights_from_parent": [
                "数据预处理：与父节点一致，无变化",
                "特征工程：将mean pooling替换为可学习的注意力加权池化，动态强调重要token",
                "模型选择：新增注意力池化层(Linear→Tanh→Linear→softmax)，改变文档表示生成方式",
                "参数调整：引入注意力层参数并保留dropout与weight_decay，模型容量略增",
                "训练策略：训练流程与父节点相同(OneCycleLR、梯度裁剪、相同batch与epoch设置)"
            ]
        }
    ]
]